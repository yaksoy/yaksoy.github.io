<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Intrinsic Harmonization for Illumination-Aware Compositing &#8211; Yağız Aksoy</title>
<meta name="description" content="Intrinsic Harmonization for Illumination-Aware Compositing">



<!-- Twitter Cards -->
<meta name="twitter:title" content="Intrinsic Harmonization for Illumination-Aware Compositing">
<meta name="twitter:description" content="Intrinsic Harmonization for Illumination-Aware Compositing">
<meta name="twitter:site" content="@yagizaksoy">
<meta name="twitter:creator" content="@yagizaksoy">

<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://yaksoy.github.io/images/intrinsicCompositingTeaser.jpg">

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Intrinsic Harmonization for Illumination-Aware Compositing">
<meta property="og:description" content="Intrinsic Harmonization for Illumination-Aware Compositing">
<meta property="og:url" content="https://yaksoy.github.io/intrinsicCompositing/">
<meta property="og:site_name" content="Yağız Aksoy">

<!-- Webmaster Tools verfication -->
<meta name="google-site-verification" content="googleb0479c04a25255c3">



<link rel="canonical" href="https://yaksoy.github.io/intrinsicCompositing/">
<link href="https://yaksoy.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Yağız Aksoy Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="https://yaksoy.github.io/assets/css/main.css">
<!-- Webfonts -->
<script src="//use.edgefonts.net/source-sans-pro:n2,i2,n3,i3,n4,i4,n6,i6,n7,i7,n9,i9;source-code-pro:n4,n7;volkhov.js"></script>

<meta http-equiv="cleartype" content="on">

<!-- HTML5 Shiv and Media Query Support -->
<!--[if lt IE 9]>
  <script src="https://yaksoy.github.io/assets/js/vendor/html5shiv.min.js"></script>
  <script src="https://yaksoy.github.io/assets/js/vendor/respond.min.js"></script>
<![endif]-->

<!-- Modernizr -->
<script src="https://yaksoy.github.io/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="https://yaksoy.github.io/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="https://yaksoy.github.io/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="https://yaksoy.github.io/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="https://yaksoy.github.io/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="https://yaksoy.github.io/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://yaksoy.github.io/images/apple-touch-icon-144x144-precomposed.png">

</head>

<body id="page">

<div id="main" role="main">
  <article class="entry">
    <div class="entry-wrapper">
      <br>
      <div class="entry-titleLogos">
        <table><colgroup><col style="width:50%" /><col style="width:50%" /></colgroup><tr><td style="text-align:left"><a href="https://www.sfu.ca/computing.html" target="_blank"><img src="../images/sfu.png" class="proj-logo" title="SFU"></a></td><td style="text-align:right"><a href="../group/" target="_blank"><img src="../images/cplab-full.png" class="proj-logo" title="CPLab" style="float:right;"></a></td></tr></table>
      </div>
      <br>
      <!-- <header class="entry-header">-->
        <h1 class="entry-titleProject" style="margin-top:5px;">Intrinsic Harmonization for Illumination-Aware Compositing</h1>
      <!-- </header>-->
      <div class="entry-titleAuthors">
        <table style="text-align:center;"><colgroup><col style="width:33%" /><col style="width:33%" /><col style="width:33%" /><td><a href="https://ccareaga.github.io" target="_blank"><img src="../group/chris.jpg" class="proj-photo" title="Chris"></a></td><td><a href="http://miangoleh.github.io/" target="_blank"><img src="../group/mahdi.jpg" class="proj-photo" title="Mahdi"></a></td><td><a href="../" target="_blank"><img src="../images/yagizsq2.jpg" class="proj-photo" title="Yagiz"></a></td></tr><tr><td><a href="https://ccareaga.github.io" target="_blank">Chris Careaga</a></td><td><a href="http://miangoleh.github.io/" target="_blank">S. Mahdi H. Miangoleh</a></td><td> <a href="../" target="_blank">Yağız Aksoy</a></td></tr></table>
      </div>
      <div class="entry-titleVenue">
        Proc. SIGGRAPH Asia, 2023
      </div>
    </div><!-- /.entry-wrapper -->
    <img src="https://yaksoy.github.io/images/intrinsicCompositingTeaser.jpg" class="entry-feature-image" alt="Intrinsic Harmonization for Illumination-Aware Compositing" style="margin-top:-40;"><p class="image-caption">We propose an illumination-aware image harmonization approach for in-the-wild imagery. Our method is formulated in the intrinsic image domain. We use off-the-shelf networks to generate albedo, shading and surface normals for the input composite and background image. We first harmonize the albedo of the background and foreground by predicting image editing parameters. Using normals and shading we estimate a simple lighting model for the background illumination. With this lighting model, we render Lambertian shading for the foreground and refine it using a network trained on segmentation datasets via self-supervision. When compared to prior works we are the only method that is capable of modeling realistic lighting effects.</p>
    <div class="entry-wrapper">
      <div class="entry-contentProject">
        <p style="text-align:center;"><b>Abstract</b></p>

<p>
Despite significant advancements in network-based image harmonization techniques, there still exists a domain disparity between typical training pairs and real-world composites encountered during inference. Most existing methods are trained to reverse global edits made on segmented image regions, which fail to accurately capture the lighting inconsistencies between the foreground and background found in composited images. In this work, we introduce a self-supervised illumination harmonization approach formulated in the intrinsic image domain. First, we estimate a simple global lighting model from mid-level vision representations to generate a rough shading for the foreground region. A network then refines this inferred shading to generate a harmonious re-shading that aligns with the background scene. In order to match the color appearance of the foreground and background, we utilize ideas from prior harmonization approaches to perform parameterized image edits in the albedo domain. To validate the effectiveness of our approach, we present results from challenging real-world composites and conduct a user study to objectively measure the enhanced realism achieved compared to state-of-the-art harmonization methods. 
</p>

<p style="text-align:center;"><b>Implementation</b></p>

<p style="text-align:center;"><a href="https://github.com/compphoto/IntrinsicCompositing" target="_blank">GitHub Repository <i class="fa fa-external-link"></i></a></p>

<p style="text-align:center;"><b>Video</b></p>

<p style="text-align:center;">
<iframe width="560" height="315" src="https://www.youtube.com/embed/M9hCUTp8bo4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</p>

<p style="text-align:center;"><b>Paper</b></p>
<table style="text-align:center;"><tr>
		<td><a href="https://yaksoy.github.io/papers/SigAsia23-IntrinsicCompositing.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/SigAsia23Paper.jpg" title="Paper" /></a></td>
		<td><a href="https://yaksoy.github.io/papers/SigAsia23-IntrinsicCompositing-Supp.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/SigAsia23Supp.jpg" title="Supplementary" /></a></td>
	</tr></table>

<p><!-- <table style="text-align:center;"><tr>
		<td><a href="" target="_blank"><i class=" fa-file-archive-o-5x"></i></a></td>
        </tr><tr>
		<td><a href="" target="_blank">Supplementary Results</a></td>
	</tr></table> --></p>

<p style="text-align:center;"><b>BibTeX</b></p>
<p>
<div class="pub-bibtex">
		@INPROCEEDINGS{careagaCompositing,<br />
		author={Chris Careaga and S. Mahdi H. Miangoleh and Ya\u{g}{\i}z Aksoy},<br />
		title={Intrinsic Harmonization for Illumination-Aware Compositing},<br />
		booktitle={Proc. SIGGRAPH Asia},<br />
		year={2023},<br />
		}
</div>
</p>

<p style="text-align:center;"><b>License</b></p>

<p>The methodology presented in this work is safeguarded under intellectual property protection. For inquiries regarding licensing opportunities, kindly reach out to SFU Technology Licensing Office &#60;tlo_dir <i>ατ</i> sfu <i>δøτ</i> ca&#62; and Dr. Yağız Aksoy &#60;yagiz <i>ατ</i> sfu <i>δøτ</i> ca&#62;.</p>

<p style="text-align:center;"><b>Related Publications</b></p>

<hr />

<table>
	<tr>
	<td class="pub-photocol">
	<a href="../intrinsic" target="_blank"><img src="https://yaksoy.github.io/images/research/intrinsic.jpg" class="pub-photo" /></a>
	</td>
	<td>
	<div class="pub-title">
		<a href="../intrinsic" target="_blank">Intrinsic Image Decomposition via Ordinal Shading <i class="fa fa-external-link"></i></a>
	</div>
	<div class="pub-authors">
		<b>Chris Careaga</b> and <b>Yağız Aksoy</b>
	</div>
	<div class="pub-venue">
		ACM Transactions on Graphics, 2023
	</div>
	
	<div class="pub-accordion">
	  <input id="TOG23-item1" name="accordion1" type="checkbox" />
	  <label for="TOG23-item1">Abstract</label>
	  <div class="pub-abstract">
		Intrinsic decomposition is a fundamental mid-level vision problem that plays a crucial role in various inverse rendering and computational photography pipelines. 
		Generating highly accurate intrinsic decompositions is an inherently under-constrained task that requires precisely estimating continuous-valued shading and albedo. 
		In this work, we achieve high-resolution intrinsic decomposition by breaking the problem into two parts. 
		First, we present a dense ordinal shading formulation using a shift- and scale-invariant loss in order to estimate ordinal shading cues without restricting the predictions to obey the intrinsic model. 
		We then combine low- and high-resolution ordinal estimations using a second network to generate a shading estimate with both global coherency and local details. 
		We encourage the model to learn an accurate decomposition by computing losses on the estimated shading as well as the albedo implied by the intrinsic model. 
		We develop a straightforward method for generating dense pseudo ground truth using our models predictions and multi-illumination data, enabling generalization to in-the-wild imagery. 
		We present exhaustive qualitative and quantitative analysis of our predicted intrinsic components against state-of-the-art methods. 
		Finally, we demonstrate the real-world applicability of our estimations by performing otherwise difficult editing tasks such as recoloring and relighting.
	  </div>
	
	  <input id="TOG23-item2" name="accordion1" type="checkbox" />
	  <label for="TOG23-item2">Manuscript &amp; more</label>
	  <div class="pub-photolink">
	  
		<table><tr>
			<td><a href="https://yaksoy.github.io/papers/TOG23-Intrinsic.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/TOG23Paper.jpg" title="Paper" /></a></td>
			<td><a href="https://yaksoy.github.io/papers/TOG23-Intrinsic-Supp.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/TOG23Supp.jpg" title="Supplementary" /></a></td>
			<td><a href="https://youtu.be/pWtJd3hqL3c" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/TOG23Video.jpg" title="Video" /></a></td>
		</tr></table>
		</div>
	
	  <input id="TOG23-item3" name="accordion1" type="checkbox" />
	  <label for="TOG23-item3">BibTeX</label>
	  <div class="pub-bibtex">
		@ARTICLE{careagaIntrinsic,<br />
		author={Chris Careaga and Ya\u{g}{\i}z Aksoy},<br />
		title={Intrinsic Image Decomposition via Ordinal Shading},<br />
		journal={ACM Trans. Graph.},<br />
		year={2023},<br />
	  }
	  </div>
	</div>
	</td>
	</tr>
	</table>

<hr />

<table>
<tr>
<td class="pub-photocol">
<a href="../realisticEditing" target="_blank"><img src="https://yaksoy.github.io/images/research/realisticEditing.jpg" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="../realisticEditing" target="_blank">Realistic Saliency Guided Image Enhancement <i class="fa fa-external-link"></i></a>
</div>
<div class="pub-authors">
	<i><b>S. Mahdi H. Miangoleh</b></i>, Zoya Bylinskii, Eric Kee, Eli Shechtman, and <b>Yağız Aksoy</b> 
</div>
<div class="pub-venue">
	CVPR, 2023
</div>

<div class="pub-accordion">
  <input id="CVPR23a-item1" name="accordion1" type="checkbox" />
  <label for="CVPR23a-item1">Abstract</label>
  <div class="pub-abstract">
	Common editing operations performed by professional photographers include the cleanup operations: de-emphasizing distracting elements and enhancing subjects. 
	These edits are challenging, requiring a delicate balance between manipulating the viewer's attention while maintaining photo realism. 
	While recent approaches can boast successful examples of attention attenuation or amplification, most of them also suffer from frequent unrealistic edits. 
	We propose a realism loss for saliency-guided image enhancement to maintain high realism across varying image types, while attenuating distractors and amplifying objects of interest. 
	Evaluations with professional photographers confirm that we achieve the dual objective of realism and effectiveness, and outperform the recent approaches on their own datasets, while requiring a smaller memory footprint and runtime. We thus offer a viable solution for automating image enhancement and photo cleanup operations. 
</div>

  <input id="CVPR23a-item2" name="accordion1" type="checkbox" />
  <label for="CVPR23a-item2">Manuscript &amp; more</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="https://yaksoy.github.io/papers/CVPR23-RealisticEditing.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/CVPR23aPaper.jpg" title="Paper" /></a></td>
		<td><a href="https://yaksoy.github.io/papers/CVPR23-RealisticEditing-Supp.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/CVPR23aSupp.jpg" title="Paper" /></a></td>
		<td><a href="https://youtu.be/5dKUDMnnjuo" target="_blank"><img src="https://yaksoy.github.io/images/research/CVPR23aVideo.jpg" title="Video" /></a></td>
	</tr></table>
	</div>

  <input id="CVPR23a-item3" name="accordion1" type="checkbox" />
  <label for="CVPR23a-item3">BibTeX</label>
  <div class="pub-bibtex">
@INPROCEEDINGS{Miangoleh2023Realistic,<br />
	author={S. Mahdi H. Miangoleh and Zoya Bylinskii and Eric Kee and Eli Shechtman and Ya\u{g}{\i}z Aksoy},<br />
	title={Realistic Saliency Guided Image Enhancement},<br />
	journal={Proc. CVPR},<br />
	year={2023},<br />
	} 
  </div>
</div>
</td>
</tr>
</table>

<hr />

<table>
	<tr>
	<td class="pub-photocol">
	<a href="../intrinsicFlash" target="_blank"><img src="https://yaksoy.github.io/images/research/intrinsicFlash.jpg" class="pub-photo" /></a>
	</td>
	<td>
	<div class="pub-title">
		<a href="../intrinsicFlash" target="_blank">Computational Flash Photography through Intrinsics <i class="fa fa-external-link"></i></a>
	</div>
	<div class="pub-authors">
		<b>Sepideh Sarajian Maralan</b>, <b>Chris Careaga</b>, and <b>Yağız Aksoy</b> 
	</div>
	<div class="pub-venue">
		CVPR, 2023
	</div>
	
	<div class="pub-accordion">
	  <input id="CVPR23b-item1" name="accordion1" type="checkbox" />
	  <label for="CVPR23b-item1">Abstract</label>
	  <div class="pub-abstract">
		Flash is an essential tool as it often serves as the sole controllable light source in everyday photography. 
		However, the use of flash is a binary decision at the time a photograph is captured with limited control over its characteristics such as strength or color. 
		In this work, we study the computational control of the flash light in photographs taken with or without flash. 
		We present a physically motivated intrinsic formulation for flash photograph formation and develop flash decomposition and generation methods for flash and no-flash photographs, respectively. 
		We demonstrate that our intrinsic formulation outperforms alternatives in the literature and allows us to computationally control flash in in-the-wild images.
	</div>
	
	  <input id="CVPR23b-item2" name="accordion1" type="checkbox" />
	  <label for="CVPR23b-item2">Manuscript &amp; more</label>
	  <div class="pub-photolink">
	  
		<table><tr>
			<td><a href="https://yaksoy.github.io/papers/CVPR23-IntrinsicFlash.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/CVPR23bPaper.jpg" title="Paper" /></a></td>
			<td><a href="https://yaksoy.github.io/papers/CVPR23-IntrinsicFlash-Dataset.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/CVPR23bSupp1.jpg" title="Dataset" /></a></td>
			<td><a href="https://yaksoy.github.io/papers/CVPR23-IntrinsicFlash-SuppResults.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/CVPR23bSupp2.jpg" title="Supplementary" /></a></td>
			<td><a href="https://youtu.be/Zs23PKgJCO8" target="_blank"><img src="https://yaksoy.github.io/images/research/CVPR23bVideo.jpg" title="Video" /></a></td>
		</tr></table>
		</div>
	
	  <input id="CVPR23b-item3" name="accordion1" type="checkbox" />
	  <label for="CVPR23b-item3">BibTeX</label>
	  <div class="pub-bibtex">
	@INPROCEEDINGS{Maralan2023Flash,<br />
		author={Sepideh Sarajian Maralan and Chris Careaga and Ya\u{g}{\i}z Aksoy},<br />
		title={Computational Flash Photography through Intrinsics},<br />
		journal={Proc. CVPR},<br />
		year={2023},<br />
		} 
	  </div>
	</div>
	</td>
	</tr>
	</table>

<hr />

      </div><!-- /.entry-contentProject -->
    </div><!-- /.entry-wrapper -->
  </article>
</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo" class="entry-wrapper">
    

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
<span>&copy; 2023 Yagiz Aksoy</span>
  </footer>
</div><!-- /.footer-wrapper -->

<script type="text/javascript">
  var BASE_URL = 'https://yaksoy.github.io';
</script>

<!-- Include Latex style math -->
<!-- https://stackoverflow.com/questions/10987992/using-mathjax-with-jekyll -->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="https://yaksoy.github.io/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="https://yaksoy.github.io/assets/js/scripts.min.js"></script>



<!-- Start of StatCounter Code for Default Guide -->
<script type="text/javascript">
var sc_project=11195487; 
var sc_invisible=1; 
var sc_security="112d9b46"; 
var scJsHost = (("https:" == document.location.protocol) ?
"https://secure." : "http://www.");
document.write("<sc"+"ript type='text/javascript' src='" +
scJsHost+
"statcounter.com/counter/counter.js'></"+"script>");
</script>
<noscript><div class="statcounter"><a title="shopify
analytics" href="http://statcounter.com/shopify/"
target="_blank"><img class="statcounter"
src="//c.statcounter.com/11195487/0/112d9b46/1/"
alt="shopify analytics"></a></div></noscript>
<!-- End of StatCounter Code for Default Guide -->

</body>
</html>
