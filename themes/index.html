<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Research Themes &#8211; Yağız Aksoy</title>
<meta name="description" content="Research Themes">



<!-- Twitter Cards -->
<meta name="twitter:title" content="Research Themes">
<meta name="twitter:description" content="Research Themes">



<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://yaksoy.github.io/images/jordan.png">

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Research Themes">
<meta property="og:description" content="Research Themes">
<meta property="og:url" content="https://yaksoy.github.io/themes/">
<meta property="og:site_name" content="Yağız Aksoy">

<!-- Webmaster Tools verfication -->
<meta name="google-site-verification" content="googleb0479c04a25255c3">



<link rel="canonical" href="https://yaksoy.github.io/themes/">
<link href="https://yaksoy.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Yağız Aksoy Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="https://yaksoy.github.io/assets/css/main.css">
<!-- Webfonts -->
<script src="//use.edgefonts.net/source-sans-pro:n2,i2,n3,i3,n4,i4,n6,i6,n7,i7,n9,i9;source-code-pro:n4,n7;volkhov.js"></script>

<meta http-equiv="cleartype" content="on">

<!-- HTML5 Shiv and Media Query Support -->
<!--[if lt IE 9]>
  <script src="https://yaksoy.github.io/assets/js/vendor/html5shiv.min.js"></script>
  <script src="https://yaksoy.github.io/assets/js/vendor/respond.min.js"></script>
<![endif]-->

<!-- Modernizr -->
<script src="https://yaksoy.github.io/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="https://yaksoy.github.io/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="https://yaksoy.github.io/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="https://yaksoy.github.io/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="https://yaksoy.github.io/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="https://yaksoy.github.io/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://yaksoy.github.io/images/apple-touch-icon-144x144-precomposed.png">

</head>

<body id="page">

<div class="navigation-wrapper">
	<nav role="navigation" id="site-nav" class="animated drop">
	    <ul>
      
		    
		    <li><a href="https://yaksoy.github.io/" >Yağız</a></li>
		  
		    
		    <li><a href="https://yaksoy.github.io/group/" >CPLab</a></li>
		  
		    
		    <li><a href="https://yaksoy.github.io/media/" >Media</a></li>
		  
		    
		    <li><a href="https://yaksoy.github.io/themes/" >Research</a></li>
		  
		    
		    <li><a href="https://yaksoy.github.io/research/" >Papers</a></li>
		  
		    
		    <li><a href="https://yaksoy.github.io/teaching/" >Teaching</a></li>
		  
		    
		    <li><a href="https://yaksoy.github.io/contact/" >Contact</a></li>
		  
	    </ul>
	</nav>
</div><!-- /.navigation-wrapper -->

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->

<header class="masthead">
	<div class="wrap">
  		<img src="title.jpg" width="700" alt="Yağız Aksoy logo" class="animated fadeInDown" style="border-radius:5px;">
      <h1 class="site-title animated fadeIn"><a href="https://yaksoy.github.io/">Yağız Aksoy</a></h1>
		<h2 class="site-description animated fadeIn" itemprop="description">Computational Photography Lab @ SFU</h2>
	</div>
</header><!-- /.masthead -->

<!-- <header class="masthead">
	<div class="wrap">
      
  		<a href="https://yaksoy.github.io/" class="site-logo" rel="home" title="Yağız Aksoy"><img src="https://yaksoy.github.io/images/cplab.jpg" width="200" height="200" alt="Yağız Aksoy logo" class="animated fadeInDown"></a>
      
      <h1 class="site-title animated fadeIn"><a href="https://yaksoy.github.io/">Yağız Aksoy</a></h1>
		<h2 class="site-description animated fadeIn" itemprop="description">Computational Photography Lab @ SFU</h2>
	</div>
</header> -->
<!-- /.masthead -->
<!--  -->

<div class="js-menu-screen menu-screen"></div>


<div id="main" role="main">
  <article class="entry">
    
    <div class="entry-wrapper">
      <header class="entry-header">
        <h1 class="entry-title" style="margin-top:-5px;">Research Themes</h1>
      </header>
      <div class="entry-content">
        <div class="homepage" style="text-align:justify;margin-top:10px;">
Computational Photography Lab specializes in physical modeling of image formation to connect computer vision and machine learning methodologies with computer graphics applications. 
Our core research explores inverse rendering and its applications to realistic image manipulation, movie post-production, and in-camera image processing.
</div>

<div class="homepage" style="text-align:justify;margin-top:10px;">
You can check out our <a href="/media/" target="_blank">Media page <i class="fa fa-external-link"></i></a>  for a more non-technical description of our work and our <a href="/research/" target="_blank">Papers page <i class="fa fa-external-link"></i></a>  for a full list of our publications.
</div>

<p><br /></p>

<table style="text-align:center;"><tr>
		<td><img width="700" src="Theme-Mid-level.jpg" title="Mid-level Vision" /></td>
	</tr></table>

<div class="homepage" style="text-align:justify;">
Mid-level vision, or inverse rendering, is the understanding of physical properties of a scene from photographs, these properties including geometry, materials, illumination, and objects. Our work cover mid-level vision problems from many aspects, always with a focus on high-resolution estimation and in-the-wild generalization.
</div>
<div class="page-accordion">
	<input id="Theme1-item1" name="accordion1" type="checkbox" />
	<label for="Theme1-item1">Selected publications <i class="fa fa-angle-down"></i></label>
	  <div><!--  style="margin-left:-10px;"> -->
		<hr />
		<table>
	<tr>
	<td class="pub-photocol">
	<a href="../intrinsic" target="_blank"><img src="https://yaksoy.github.io/images/research/intrinsic.jpg" class="pub-photo" /></a>
	</td>
	<td>
	<div class="pub-title">
		<a href="../intrinsic" target="_blank">Intrinsic Image Decomposition via Ordinal Shading <i class="fa fa-external-link"></i></a>
	</div>
	<div class="pub-authors">
		<b>Chris Careaga</b> and <b>Yağız Aksoy</b>
	</div>
	<div class="pub-venue">
		ACM Transactions on Graphics, 2023
	</div>
	
	<div class="pub-accordion">
	  <input id="TOG23-item1" name="accordion1" type="checkbox" />
	  <label for="TOG23-item1">Abstract</label>
	  <div class="pub-abstract">
		Intrinsic decomposition is a fundamental mid-level vision problem that plays a crucial role in various inverse rendering and computational photography pipelines. 
		Generating highly accurate intrinsic decompositions is an inherently under-constrained task that requires precisely estimating continuous-valued shading and albedo. 
		In this work, we achieve high-resolution intrinsic decomposition by breaking the problem into two parts. 
		First, we present a dense ordinal shading formulation using a shift- and scale-invariant loss in order to estimate ordinal shading cues without restricting the predictions to obey the intrinsic model. 
		We then combine low- and high-resolution ordinal estimations using a second network to generate a shading estimate with both global coherency and local details. 
		We encourage the model to learn an accurate decomposition by computing losses on the estimated shading as well as the albedo implied by the intrinsic model. 
		We develop a straightforward method for generating dense pseudo ground truth using our models predictions and multi-illumination data, enabling generalization to in-the-wild imagery. 
		We present exhaustive qualitative and quantitative analysis of our predicted intrinsic components against state-of-the-art methods. 
		Finally, we demonstrate the real-world applicability of our estimations by performing otherwise difficult editing tasks such as recoloring and relighting.
	  </div>
	
	  <input id="TOG23-item2" name="accordion1" type="checkbox" />
	  <label for="TOG23-item2">Manuscript &amp; more</label>
	  <div class="pub-photolink">
	  
		<table><tr>
			<td><a href="https://yaksoy.github.io/papers/TOG23-Intrinsic.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/TOG23Paper.jpg" title="Paper" /></a></td>
			<td><a href="https://yaksoy.github.io/papers/TOG23-Intrinsic-Supp.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/TOG23Supp.jpg" title="Supplementary" /></a></td>
			<td><a href="https://youtu.be/pWtJd3hqL3c" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/TOG23Video.jpg" title="Video" /></a></td>
			<td><a href="https://yaksoy.github.io/intrinsic/Intrinsic_Poster.jpg" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/TOG23Poster.jpg" title="Poster" /></a></td>
		</tr></table>
		</div>
	
	  <input id="TOG23-item3" name="accordion1" type="checkbox" />
	  <label for="TOG23-item3">BibTeX</label>
	  <div class="pub-bibtex">
		@ARTICLE{careagaIntrinsic,<br />
		author={Chris Careaga and Ya\u{g}{\i}z Aksoy},<br />
		title={Intrinsic Image Decomposition via Ordinal Shading},<br />
		journal={ACM Trans. Graph.},<br />
		year={2023},<br />
		volume = {43},<br />
		number = {1},<br />
		articleno = {12},<br />
		numpages = {24},<br />
	  }
	  </div>
	</div>
	</td>
	</tr>
	</table>
	  	<hr />
		<table>
	<tr>
	<td class="pub-photocol">
	<a href="../ColorfulShading" target="_blank"><img src="https://yaksoy.github.io/images/research/ColorfulShading.jpg" class="pub-photo" /></a>
	</td>
	<td>
	<div class="pub-title">
		<a href="../ColorfulShading" target="_blank">Colorful Diffuse Intrinsic Image Decomposition in the Wild <i class="fa fa-external-link"></i></a>
	</div>
	<div class="pub-authors">
		<b>Chris Careaga</b> and <b>Yağız Aksoy</b>
	</div>
	<div class="pub-venue">
		ACM Transactions on Graphics (Proc. SIGGRAPH Asia), 2024
		<br />
		<b><i>Best Paper Award Honorable Mention</i></b>
	</div>
	
	<div class="pub-accordion">
	  <input id="TOG24-item1" name="accordion1" type="checkbox" />
	  <label for="TOG24-item1">Abstract</label>
	  <div class="pub-abstract">
		Intrinsic image decomposition aims to separate the surface reflectance and the effects from the illumination given a single photograph. 
		Due to the complexity of the problem, most prior works assume a single-color illumination and a Lambertian world, which limits their use in illumination-aware image editing applications. 
		In this work, we separate an input image into its diffuse albedo, colorful diffuse shading, and specular residual components. 
		We arrive at our result by gradually removing first the single-color illumination and then the Lambertian-world assumptions. 
		We show that by dividing the problem into easier sub-problems, in-the-wild colorful diffuse shading estimation can be achieved despite the limited ground-truth datasets. 
		Our extended intrinsic model enables illumination-aware analysis of photographs and can be used for image editing applications such as specularity removal and per-pixel white balancing.
	  </div>
	
	  <input id="TOG24-item2" name="accordion1" type="checkbox" />
	  <label for="TOG24-item2">Manuscript &amp; more</label>
	  <div class="pub-photolink">
	  
		<table><tr>
			<td><a href="https://yaksoy.github.io/papers/TOG24-ColorfulShading.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/TOG24Paper.jpg" title="Paper" /></a></td>
			<td><a href="https://yaksoy.github.io/papers/TOG24-ColorfulShading-supp.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/TOG24Supp.jpg" title="Supplementary" /></a></td>
			<!-- <td><a href="https://youtu.be/pWtJd3hqL3c" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/TOG24Video.jpg" title="Video"></a></td> -->
		</tr></table>
		</div>
	
	  <input id="TOG24-item3" name="accordion1" type="checkbox" />
	  <label for="TOG24-item3">BibTeX</label>
	  <div class="pub-bibtex">
		@ARTICLE{careagaColorful,<br />
		author={Chris Careaga and Ya\u{g}{\i}z Aksoy},<br />
		title={Colorful Diffuse Intrinsic Image Decomposition in the Wild},<br />
		journal={ACM Trans. Graph.},<br />
		year={2024},<br />
		volume = {43},<br />
		number = {6},<br />
		articleno = {178},<br />
		numpages = {12},<br />
	  }
	  </div>
	</div>
	</td>
	</tr>
	</table>
		<hr />
		<table>
<tr>
<td class="pub-photocol">
<a href="../highresdepth" target="_blank"><img src="https://yaksoy.github.io/images/research/highresdepth.jpg" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="../highresdepth" target="_blank">Boosting Monocular Depth Estimation Models to High Resolution <i class="fa fa-external-link"></i></a>
</div>
<div class="pub-authors">
	<i><b>S. Mahdi H. Miangoleh*</b></i>, <i><b>Sebastian Dille*</b></i>, Long Mai, Sylvain Paris, and <b>Yağız Aksoy</b> 
</div>
<div class="pub-venue">
	CVPR, 2021
</div>

<div class="pub-accordion">
  <input id="CVPR21-item1" name="accordion1" type="checkbox" />
  <label for="CVPR21-item1">Abstract</label>
  <div class="pub-abstract">
	Neural networks have shown great abilities in estimating depth from a single image. 
	However, the inferred depth maps are well below one-megapixel resolution and often lack fine-grained details, which limits their practicality. 
	Our method builds on our analysis on how the input resolution and the scene structure affects depth estimation performance. 
	We demonstrate that there is a trade-off between a consistent scene structure and the high-frequency details, and merge low- and high-resolution estimations to take advantage of this duality using a simple depth merging network. 
	We present a double estimation method that improves the whole-image depth estimation and a patch selection method that adds local details to the final result. 
	We demonstrate that by merging estimations at different resolutions with changing context, we can generate multi-megapixel depth maps with a high level of detail using a pre-trained model.
</div>

  <input id="CVPR21-item2" name="accordion1" type="checkbox" />
  <label for="CVPR21-item2">Manuscript &amp; more</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="https://yaksoy.github.io/papers/CVPR21-HighResDepth.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/CVPR21Paper.jpg" title="Paper" /></a></td>
		<td><a href="https://yaksoy.github.io/papers/CVPR21-HighResDepth-Supp.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/CVPR21Supp.jpg" title="Paper" /></a></td>
		<td><a href="https://youtu.be/lDeI17pHlqo" target="_blank"><img src="https://yaksoy.github.io/images/research/CVPR21Video.jpg" title="Video" /></a></td>
		<td><a href="https://yaksoy.github.io/highresdepth/CVPR21PosterSm.jpg" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/CVPR21Poster.jpg" title="Poster" /></a></td>
	</tr></table>
	</div>

  <input id="CVPR21-item3" name="accordion1" type="checkbox" />
  <label for="CVPR21-item3">BibTeX</label>
  <div class="pub-bibtex">
@INPROCEEDINGS{Miangoleh2021Boosting,<br />
	author={S. Mahdi H. Miangoleh and Sebastian Dille and Long Mai and Sylvain Paris and Ya\u{g}{\i}z Aksoy},<br />
	title={Boosting Monocular Depth Estimation Models to High-Resolution via Content-Adaptive Multi-Resolution Merging},<br />
	journal={Proc. CVPR},<br />
	year={2021},<br />
	} 
  </div>
</div>
</td>
</tr>
</table>
		<hr />
		<table>
	<tr>
	<td class="pub-photocol">
	<a href="../sidepth" target="_blank"><img src="https://yaksoy.github.io/images/research/sidepth.jpg" class="pub-photo" /></a>
	</td>
	<td>
	<div class="pub-title">
		<a href="../sidepth" target="_blank">Scale-Invariant Monocular Depth Estimation via SSI Depth <i class="fa fa-external-link"></i></a>
	</div>
	<div class="pub-authors">
		<b>S. Mahdi H. Miangoleh</b>, <b>Mahesh Reddy</b>, and <b>Yağız Aksoy</b>
	</div>
	<div class="pub-venue">
		SIGGRAPH, 2024
	</div>
	
	<div class="pub-accordion">
	  <input id="Sig24-item1" name="accordion1" type="checkbox" />
	  <label for="Sig24-item1">Abstract</label>
	  <div class="pub-abstract">
		Existing methods for scale-invariant monocular depth estimation (SI MDE) often struggle due to the complexity of the task, and limited and non-diverse datasets, hindering generalizability in real-world scenarios. This is while shift-and-scale-invariant (SSI) depth estimation, simplifying the task and enabling training with abundant stereo datasets achieves high performance. We present a novel approach that leverages SSI inputs to enhance SI depth estimation, streamlining the network's role and facilitating in-the-wild generalization for SI depth estimation while only using a synthetic dataset for training. Emphasizing the generation of high-resolution details, we introduce a novel sparse ordinal loss that substantially improves detail generation in SSI MDE, addressing critical limitations in existing approaches. Through in-the-wild qualitative examples and zero-shot evaluation we substantiate the practical utility of our approach in computational photography applications, showcasing its ability to generate highly detailed SI depth maps and achieve generalization in diverse scenarios.
	  </div>
	
	  <input id="Sig24-item2" name="accordion1" type="checkbox" />
	  <label for="Sig24-item2">Manuscript &amp; more</label>
	  <div class="pub-photolink">
	  
		<table><tr>
			<td><a href="https://yaksoy.github.io/papers/SIG24-SI-Depth.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/Sig24Paper.jpg" title="Paper" /></a></td>
			<td><a href="https://yaksoy.github.io/papers/SIG24-SI-Depth-Supp.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/Sig24Supp.jpg" title="Supplementary" /></a></td>
			<td><a href="https://youtu.be/R_vW6TjYiEM" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/SIG24Video.jpg" title="Video" /></a></td>
			<td><a href="https://yaksoy.github.io/sidepth/SIDepth_Poster.jpg" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/SIG24Poster.jpg" title="Poster" /></a></td>
		</tr></table>
		</div>
	
	  <input id="Sig24-item3" name="accordion1" type="checkbox" />
	  <label for="Sig24-item3">BibTeX</label>
	  <div class="pub-bibtex">
		@INPROCEEDINGS{miangolehSIDepth,<br />
		author={S. Mahdi H. Miangoleh and Mahesh Reddy and Ya\u{g}{\i}z Aksoy},<br />
		title={Scale-Invariant Monocular Depth Estimation via SSI Depth},<br />
		booktitle={Proc. SIGGRAPH},<br />
		year={2024},<br />
	  }
	  </div>
	</div>
	</td>
	</tr>
	</table>
		<hr />
		<table>
	<tr>
	<td class="pub-photocol">
	<a href="../bottomup" target="_blank"><img src="https://yaksoy.github.io/images/research/bottomup.jpg" class="pub-photo" /></a>
	</td>
	<td>
	<div class="pub-title">
		<a href="../bottomup" target="_blank">A Bottom-Up Approach to Class-Agnostic Image Segmentation <i class="fa fa-external-link"></i></a>
	</div>
	<div class="pub-authors">
		<b>Sebastian Dille</b>, <b>Ari Blondal</b>, Sylvain Paris, and <b>Yağız Aksoy</b>
	</div>
	<div class="pub-venue">
		ECCV Workshops, 2024
	</div>
	
	<div class="pub-accordion">
	  <input id="ECCVW24-item1" name="accordion1" type="checkbox" />
	  <label for="ECCVW24-item1">Abstract</label>
	  <div class="pub-abstract">
		Class-agnostic image segmentation is a crucial component in automating image editing workflows, especially in contexts where object selection traditionally involves interactive tools. 
        Existing methods in the literature often adhere to top-down formulations, following the paradigm of class-based approaches, where object detection precedes per-object segmentation.
        In this work, we present a novel bottom-up formulation for addressing the class-agnostic segmentation problem. 
        We supervise our network directly on the projective sphere of its feature space, employing losses inspired by metric learning literature as well as losses defined in a novel segmentation-space representation.
        The segmentation results are obtained through a straightforward mean-shift clustering of the estimated features.
        Our bottom-up formulation exhibits exceptional generalization capability, even when trained on datasets designed for class-based segmentation. We further showcase the effectiveness of our generic approach by addressing the challenging task of cell and nucleus segmentation.
        We believe that our bottom-up formulation will offer valuable insights into diverse segmentation challenges in the literature.
	  </div>
	
	  <input id="ECCVW24-item2" name="accordion1" type="checkbox" />
	  <label for="ECCVW24-item2">Manuscript &amp; more</label>
	  <div class="pub-photolink">
	  
		<table><tr>
			<td><a href="https://yaksoy.github.io/papers/ECCVW24-BottomUp.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/ECCVW24Paper.jpg" title="Paper" /></a></td>
			<td><a href="https://yaksoy.github.io/papers/ECCVW24-BottomUp-supp.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/ECCVW24Supp.jpg" title="Supplementary" /></a></td>
			<!-- <td><a href="https://youtu.be/EiyH52BcKkw" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/ECCVW24Video.jpg" title="Video"></a></td> -->
			<td><a href="https://yaksoy.github.io/bottomup/BottomUpPoster.jpg" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/ECCVW24Poster.jpg" title="Poster" /></a></td>
		</tr></table>
		</div>
	
	  <input id="ECCVW24-item3" name="accordion1" type="checkbox" />
	  <label for="ECCVW24-item3">BibTeX</label>
	  <div class="pub-bibtex">
		@INPROCEEDINGS{dilleBottomup,<br />
		author={Sebastian Dille and Ari Blondal and Sylvain Paris and Ya\u{g}{\i}z Aksoy},<br />
		title={A Bottom-Up Approach to Class-Agnostic Image Segmentation},<br />
		booktitle={Proc. ECCV Workshop},<br />
		year={2024},<br />
	  }
	  </div>
	</div>
	</td>
	</tr>
	</table>
		<hr />
	  </div>
</div>

<hr />

<table style="text-align:center;"><tr>
		<td><img width="700" src="Theme-IlluminationAware.jpg" title="Illumination-Aware Computational Photography" /></td>
	</tr></table>

<div class="homepage" style="text-align:justify;">
In illumination-aware computational photography, we study physically-based models to address a wide range of photography applications. Enabled by our work intrinsic decomposition, we explore realistic manipulation of light for relighting, compositing, HDR reconstruction, and more.
</div>
<div class="page-accordion">
	<input id="Theme2-item1" name="accordion1" type="checkbox" />
	<label for="Theme2-item1">Selected publications <i class="fa fa-angle-down"></i></label>
	  <div>
	  	<hr />
		<table>
	<tr>
	<td class="pub-photocol">
	<a href="../PhysicalRelighting" target="_blank"><img src="https://yaksoy.github.io/images/research/PhysicalRelighting.gif" class="pub-photo" /></a>
	</td>
	<td>
	<div class="pub-title">
		<a href="../PhysicalRelighting" target="_blank">Physically Controllable Relighting of Photographs <i class="fa fa-external-link"></i></a>
	</div>
	<div class="pub-authors">
		<b>Chris Careaga</b> and <b>Yağız Aksoy</b>
	</div>
	<div class="pub-venue">
		Proc. SIGGRAPH, 2025
	</div>
	
	<div class="pub-accordion">
	  <input id="SIG25-item1" name="accordion1" type="checkbox" />
	  <label for="SIG25-item1">Abstract</label>
	  <div class="pub-abstract">
		We present a self-supervised approach to in-the-wild image relighting that enables fully controllable, physically based illumination editing.
		We achieve this by combining the physical accuracy of traditional rendering with the photorealistic appearance made possible by neural rendering.
		Our pipeline works by inferring a colored mesh representation of a given scene using monocular estimates of geometry and intrinsic components.
		This representation allows users to define their desired illumination configuration in 3D. The scene under the new lighting can then be rendered using a path-tracing engine.
		We send this approximate rendering of the scene through a feed-forward neural renderer to predict the final photorealistic relighting result.
		We develop a differentiable rendering process to reconstruct in-the-wild scene illumination, enabling self-supervised training of our neural renderer on raw image collections.
		Our method represents a significant step in bringing the explicit physical control over lights available in typical 3D computer graphics tools, such as Blender, to in-the-wild relighting.
	  </div>
	
	  <input id="SIG25-item2" name="accordion1" type="checkbox" />
	  <label for="SIG25-item2">Manuscript &amp; more</label>
	  <div class="pub-photolink">
	  
		<table><tr>
			<td><a href="https://yaksoy.github.io/papers/SIG25-PhysicalRelighting.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/Sig25Paper.jpg" title="Paper" /></a></td>
			<td><a href="https://yaksoy.github.io/papers/SIG25-PhysicalRelighting-Supp.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/Sig25Supp.jpg" title="Supplementary" /></a></td>
			<td><a href="https://youtu.be/XFJCT3D8t0M" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/Sig25Video.jpg" title="Video" /></a></td>
			<td><a href="https://yaksoy.github.io/PhysicalRelighting/relight_poster.jpg" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/Sig25Poster.jpg" title="Poster" /></a></td>
		</tr></table>
		</div>
	
	  <input id="SIG25-item3" name="accordion1" type="checkbox" />
	  <label for="SIG25-item3">BibTeX</label>
	  <div class="pub-bibtex">
		  @INPROCEEDINGS{careagaRelighting,<br />
		  author={Chris Careaga and Ya\u{g}{\i}z Aksoy},<br />
		  title={Physically Controllable Relighting of Photographs},<br />
		  booktitle={Proc. SIGGRAPH},<br />
		  year={2025},<br />
	  } 
	  </div>
	</div>
	</td>
	</tr>
	</table>
		<hr />
		<table>
	<tr>
	<td class="pub-photocol">
	<a href="../intrinsicHDR" target="_blank"><img src="https://yaksoy.github.io/images/research/intrinsicHDR.jpg" class="pub-photo" /></a>
	</td>
	<td>
	<div class="pub-title">
		<a href="../intrinsicHDR" target="_blank">Intrinsic Single-Image HDR Reconstruction <i class="fa fa-external-link"></i></a>
	</div>
	<div class="pub-authors">
		<b>Sebastian Dille*</b>, <b>Chris Careaga*</b>, and <b>Yağız Aksoy</b>
	</div>
	<div class="pub-venue">
		ECCV, 2024
	</div>
	
	<div class="pub-accordion">
	  <input id="ECCV24-item1" name="accordion1" type="checkbox" />
	  <label for="ECCV24-item1">Abstract</label>
	  <div class="pub-abstract">
		The low dynamic range (LDR) of common cameras fails to capture the rich contrast in natural scenes, resulting in loss of color and details in saturated pixels. 
		Reconstructing the high dynamic range (HDR) of luminance present in the scene from single LDR photographs is an important task with many applications in computational photography and realistic display of images.
		The HDR reconstruction task aims to infer the lost details using the context present in the scene, requiring neural networks to understand high-level geometric and illumination cues. 
		This makes it challenging for data-driven algorithms to generate accurate and high-resolution results.
		In this work, we introduce a physically-inspired remodeling of the HDR reconstruction problem in the intrinsic domain. 
		The intrinsic model allows us to train separate networks to extend the dynamic range in the shading domain and to recover lost color details in the albedo domain. 
		We show that dividing the problem into two simpler sub-tasks improves performance in a wide variety of photographs.
	  </div>
	
	  <input id="ECCV24-item2" name="accordion1" type="checkbox" />
	  <label for="ECCV24-item2">Manuscript &amp; more</label>
	  <div class="pub-photolink">
	  
		<table><tr>
			<td><a href="https://yaksoy.github.io/papers/ECCV24-IntrinsicHDR.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/ECCV24Paper.jpg" title="Paper" /></a></td>
			<td><a href="https://yaksoy.github.io/papers/ECCV24-IntrinsicHDR-supp.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/ECCV24Supp.jpg" title="Supplementary" /></a></td>
			<td><a href="https://youtu.be/EiyH52BcKkw" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/ECCV24Video.jpg" title="Video" /></a></td>
			<td><a href="https://yaksoy.github.io/intrinsicHDR/IntrinsicHDRPoster.jpg" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/ECCV24Poster.jpg" title="Poster" /></a></td>
		</tr></table>
		</div>
	
	  <input id="ECCV24-item3" name="accordion1" type="checkbox" />
	  <label for="ECCV24-item3">BibTeX</label>
	  <div class="pub-bibtex">
		@INPROCEEDINGS{dilleIntrinsicHDR,<br />
		author={Sebastian Dille and Chris Careaga and Ya\u{g}{\i}z Aksoy},<br />
		title={Intrinsic Single-Image HDR Reconstruction},<br />
		booktitle={Proc. ECCV},<br />
		year={2024},<br />
	  }
	  </div>
	</div>
	</td>
	</tr>
	</table>
		<hr />
		<table>
	<tr>
	<td class="pub-photocol">
	<a href="../intrinsicCompositing" target="_blank"><img src="https://yaksoy.github.io/images/research/intrinsicCompositing.jpg" class="pub-photo" /></a>
	</td>
	<td>
	<div class="pub-title">
		<a href="../intrinsicCompositing" target="_blank">Intrinsic Harmonization for Illumination-Aware Compositing <i class="fa fa-external-link"></i></a>
	</div>
	<div class="pub-authors">
		<b>Chris Careaga</b>, <b>S. Mahdi H. Miangoleh</b>, and <b>Yağız Aksoy</b>
	</div>
	<div class="pub-venue">
		SIGGRAPH Asia, 2023
	</div>
	
	<div class="pub-accordion">
	  <input id="SigAsia23-item1" name="accordion1" type="checkbox" />
	  <label for="SigAsia23-item1">Abstract</label>
	  <div class="pub-abstract">
		Despite significant advancements in network-based image harmonization techniques, there still exists a domain disparity between typical training pairs and real-world composites encountered during inference. 
		Most existing methods are trained to reverse global edits made on segmented image regions, which fail to accurately capture the lighting inconsistencies between the foreground and background found in composited images. 
		In this work, we introduce a self-supervised illumination harmonization approach formulated in the intrinsic image domain. 
		First, we estimate a simple global lighting model from mid-level vision representations to generate a rough shading for the foreground region. 
		A network then refines this inferred shading to generate a harmonious re-shading that aligns with the background scene. 
		In order to match the color appearance of the foreground and background, we utilize ideas from prior harmonization approaches to perform parameterized image edits in the albedo domain. 
		To validate the effectiveness of our approach, we present results from challenging real-world composites and conduct a user study to objectively measure the enhanced realism achieved compared to state-of-the-art harmonization methods. 
	  </div>
	
	  <input id="SigAsia23-item2" name="accordion1" type="checkbox" />
	  <label for="SigAsia23-item2">Manuscript &amp; more</label>
	  <div class="pub-photolink">
	  
		<table><tr>
			<td><a href="https://yaksoy.github.io/papers/SigAsia23-IntrinsicCompositing.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/SigAsia23Paper.jpg" title="Paper" /></a></td>
			<td><a href="https://yaksoy.github.io/papers/SigAsia23-IntrinsicCompositing-Supp.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/SigAsia23Supp.jpg" title="Supplementary" /></a></td>
			<td><a href="https://youtu.be/M9hCUTp8bo4" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/SigAsia23Video.jpg" title="Video" /></a></td>
		</tr></table>
		</div>
	
	  <input id="SigAsia23-item3" name="accordion1" type="checkbox" />
	  <label for="SigAsia23-item3">BibTeX</label>
	  <div class="pub-bibtex">
		@INPROCEEDINGS{careagaCompositing,<br />
		author={Chris Careaga and S. Mahdi H. Miangoleh and Ya\u{g}{\i}z Aksoy},<br />
		title={Intrinsic Harmonization for Illumination-Aware Compositing},<br />
		booktitle={Proc. SIGGRAPH Asia},<br />
		year={2023},<br />
	  }
	  </div>
	</div>
	</td>
	</tr>
	</table>
		<hr />
		<table>
	<tr>
	<td class="pub-photocol">
	<a href="../intrinsicFlash" target="_blank"><img src="https://yaksoy.github.io/images/research/intrinsicFlash.jpg" class="pub-photo" /></a>
	</td>
	<td>
	<div class="pub-title">
		<a href="../intrinsicFlash" target="_blank">Computational Flash Photography through Intrinsics <i class="fa fa-external-link"></i></a>
	</div>
	<div class="pub-authors">
		<b>Sepideh Sarajian Maralan</b>, <b>Chris Careaga</b>, and <b>Yağız Aksoy</b> 
	</div>
	<div class="pub-venue">
		CVPR, 2023
	</div>
	
	<div class="pub-accordion">
	  <input id="CVPR23b-item1" name="accordion1" type="checkbox" />
	  <label for="CVPR23b-item1">Abstract</label>
	  <div class="pub-abstract">
		Flash is an essential tool as it often serves as the sole controllable light source in everyday photography. 
		However, the use of flash is a binary decision at the time a photograph is captured with limited control over its characteristics such as strength or color. 
		In this work, we study the computational control of the flash light in photographs taken with or without flash. 
		We present a physically motivated intrinsic formulation for flash photograph formation and develop flash decomposition and generation methods for flash and no-flash photographs, respectively. 
		We demonstrate that our intrinsic formulation outperforms alternatives in the literature and allows us to computationally control flash in in-the-wild images.
	</div>
	
	  <input id="CVPR23b-item2" name="accordion1" type="checkbox" />
	  <label for="CVPR23b-item2">Manuscript &amp; more</label>
	  <div class="pub-photolink">
	  
		<table><tr>
			<td><a href="https://yaksoy.github.io/papers/CVPR23-IntrinsicFlash.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/CVPR23bPaper.jpg" title="Paper" /></a></td>
			<td><a href="https://yaksoy.github.io/papers/CVPR23-IntrinsicFlash-Dataset.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/CVPR23bSupp1.jpg" title="Dataset" /></a></td>
			<td><a href="https://yaksoy.github.io/papers/CVPR23-IntrinsicFlash-SuppResults.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/CVPR23bSupp2.jpg" title="Supplementary" /></a></td>
			<td><a href="https://youtu.be/Zs23PKgJCO8" target="_blank"><img src="https://yaksoy.github.io/images/research/CVPR23bVideo.jpg" title="Video" /></a></td>
			<td><a href="https://yaksoy.github.io/intrinsicFlash/CVPR23bPosterSm.jpg" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/CVPR23bPoster.jpg" title="Poster" /></a></td>
		</tr></table>
		</div>
	
	  <input id="CVPR23b-item3" name="accordion1" type="checkbox" />
	  <label for="CVPR23b-item3">BibTeX</label>
	  <div class="pub-bibtex">
	@INPROCEEDINGS{Maralan2023Flash,<br />
		author={Sepideh Sarajian Maralan and Chris Careaga and Ya\u{g}{\i}z Aksoy},<br />
		title={Computational Flash Photography through Intrinsics},<br />
		journal={Proc. CVPR},<br />
		year={2023},<br />
		} 
	  </div>
	</div>
	</td>
	</tr>
	</table>
		<hr />
	  </div>
</div>

<hr />

<table style="text-align:center;"><tr>
		<td><img width="700" src="Theme-Studio.jpg" title="Computational Photography Studio" /></td>
	</tr></table>

<div class="homepage" style="text-align:justify;">
We recently set up our custom computational photography research studio to conduct research in an active production environment. Our current hands-on research explores modeling of light, physical image formation, inverse rendering, and differentiable rendering through controlled capture setups. Stay tuned.
</div>
<div class="page-accordion">
	<input id="Theme3-item1" name="accordion1" type="checkbox" />
	<label for="Theme3-item1">Learn more about our studio <i class="fa fa-angle-down"></i></label>
	  <div>
	  	<hr />
		<table>
    <colgroup>
      <col style="width: 25%" />
      <col style="width: 75%" />
    </colgroup>
  <tr>
  <td style="max-width:150px;">
  <a href="https://www.sfu.ca/sfunews/stories/2024/07/sfu-lab-develops-ai-enabled-post-production-tools-for-independent-filmmakers.html" target="_blank"><img src="https://yaksoy.github.io/images/media/sfuinterview.jpg" class="pub-photo" title="Yagiz" /></a>
  </td>
  <td>
  <div class="pub-title">
      <a href="https://www.sfu.ca/sfunews/stories/2024/07/sfu-lab-develops-ai-enabled-post-production-tools-for-independent-filmmakers.html" target="_blank">SFU lab develops AI-enabled post-production tools for independent Canadian filmmakers
   <i class="fa fa-external-link"></i></a>
  </div>
  <div class="pub-authors">
      Simon Fraser University, 2024
  </div>
  <div class="pub-venue">
      <a href="https://www.youtube.com/playlist?list=PLj_KB0_j7uiuK5m5I5MT7MKhfVN-5yghB" target="_blank">Full interview with Yağız Aksoy
   <i class="fa fa-external-link"></i></a>
  </div>
  </td>
  </tr>
  </table>
		<hr />
		<table>
<tr>
<td class="pub-photocol">
<a href="https://yaksoy.github.io/studio-msc/" target="_blank"><img src="https://yaksoy.github.io/images/research/obummsc.jpg" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="https://yaksoy.github.io/studio-msc/" target="_blank">Setting up a Computational Photography Research Studio <i class="fa fa-external-link"></i></a>
</div>
<div class="pub-authors">
	<b>Obumneme Stanley Dukor</b>
</div>
<div class="pub-venue">
	MSc Thesis, Simon Fraser University, 2024
</div>

<div class="pub-accordion">
	<input id="SFU24-item1" name="accordion1" type="checkbox" />
	<label for="SFU24-item1">Abstract</label>
	<div class="pub-abstract">
		AI research is transforming creative tasks, with advancements in AI tools rapidly changing post-production expectations. However, the development of these technologies is mostly driven by technologists, often without involving the creatives who will use them. This thesis presents the development of a Computational Photography Research Studio aimed at bridging this gap. The goal is to create a practical and flexible studio setup that allows collaboration between creatives and researchers, allowing production and research to occur simultaneously. This new type of research involves stakeholders, like filmmakers, to ensure the research addresses their needs and benefits creative professionals. The studio setup includes portable production cameras and lighting, enabling the capture of high-quality live-action footage and datasets necessary for developing computational photography algorithms for post-production. This environment aims to direct AI research to better serve the filmmaking community, ultimately enhancing the quality of visual storytelling.
  </div>
  
  <input id="SFU24-item2" name="accordion1" type="checkbox" />
  <label for="SFU24-item2">Thesis and Presentation</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="https://www.stanleydukor.com/static/media/MSc_Thesis.664faf9d.pdf" target="_blank"><img src="https://yaksoy.github.io/images/research/SFU24ObumPaper.jpg" title="Paper" /></a></td>
		<td><a href="https://youtu.be/VeDQ_Apicgo" target="_blank"><img src="https://yaksoy.github.io/images/research/SFU24ObumVideo.jpg" title="Video" /></a></td>
	</tr></table>
	</div>
  
  <input id="SFU24-item3" name="accordion1" type="checkbox" />
  <label for="SFU24-item3">BibTeX</label>
  <div class="pub-bibtex">
	@MASTERSTHESIS{studio-msc,<br />
		author={Obumneme Stanley Dukor},<br />
		title={Setting up a Computational Photography Research Studioh},<br />
		year={2024},<br />
		school={Simon Fraser University},<br />
  }
  </div>
</div>
</td>
</tr>
</table>
		<hr />
	  </div>
</div>

<hr />

<table style="text-align:center;"><tr>
		<td><img width="700" src="Theme-Applications.jpg" title="Computational Photography Applications" /></td>
	</tr></table>

<div class="homepage" style="text-align:justify;">
Our work on mid-level vision and illumination-aware computational photography enable many immediate applications in realistic, physically-based image manipulation. We explore these applications through semester projects in <a href="/cpim/" target="_blank">CMPT 461/769 <i class="fa fa-external-link"></i></a> supervised by CPLab members.
</div>
<div class="page-accordion">
	<input id="Theme4-item1" name="accordion1" type="checkbox" />
	<label for="Theme4-item1">Selected publications <i class="fa fa-angle-down"></i></label>
	  <div>
		<hr />
		<table>
<tr>
<td class="pub-photocol">
	<a href="../2DGraphicsComp" target="_blank"><img src="https://yaksoy.github.io/images/research/2025Poster-LogoCompositing.jpg" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="../2DGraphicsComp" target="_blank">Physically-Based Compositing of 2D Graphics <i class="fa fa-external-link"></i></a>
</div>
<div class="pub-authors">
	Tyrus Tracey, Stefan Diaconu, <b>Sebastian Dille</b>, <b>S. Mahdi H. Miangoleh</b>, and <b>Yağız Aksoy</b> 
</div>
<div class="pub-venue">
	SIGGRAPH Posters, 2025
</div>

<div class="pub-accordion">
  <input id="SIG25b-item1" name="accordion1" type="checkbox" />
  <label for="SIG25b-item1">Abstract</label>
  <div class="pub-abstract">
	We propose an interactive pipeline that enables the seamless integration of a 2D logo into a target image, adapting to the surface geometry and lighting conditions of the scene to ensure realistic appearance.
  </div>

  <input id="SIG25b-item2" name="accordion1" type="checkbox" />
  <label for="SIG25b-item2">Manuscript &amp; more</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="https://yaksoy.github.io/papers/SIG25p-2DGraphicCompositing.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/2025Posterb-Paper.jpg" title="Paper" /></a></td>
		<td><a href="https://yaksoy.github.io/papers/SIG25p-2DGraphicCompositing.jpg" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/2025Posterb-Poster.jpg" title="Poster" /></a></td>
		<td><a href="https://youtu.be/Z8e22OwXgEs" target="_blank"><img src="https://yaksoy.github.io/images/research/2025Posterb-Video.jpg" title="Video" /></a></td>
	</tr></table>
	</div>

  <input id="SIG25b-item3" name="accordion1" type="checkbox" />
  <label for="SIG25b-item3">BibTeX</label>
  <div class="pub-bibtex">
	@INPROCEEDINGS{traceyDiaconuCompositing,<br />
	author={Tyrus Tracey and Stefan Diaconu and Sebastian Dille and S. Mahdi H. Miangoleh and Ya\u{g}{\i}z Aksoy},<br />
	title={Physically-Based Compositing of {2D} Graphics},<br />
	booktitle={SIGGRAPH Posters},<br />
	year={2025},<br />
	}
  </div>
</div>
</td>
</tr>
</table>
		<hr />
		<table>
<tr>
<td class="pub-photocol">
	<a href="../DiffLightComp" target="_blank"><img src="https://yaksoy.github.io/images/research/2025Poster-LightCompositing.jpg" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="../DiffLightComp" target="_blank">Interactive Object Insertion with Differentiable Rendering <i class="fa fa-external-link"></i></a>
</div>
<div class="pub-authors">
	Weikun Peng*, Sota Taira*, <b>Chris Careaga</b>, and <b>Yağız Aksoy</b> 
</div>
<div class="pub-venue">
	SIGGRAPH Posters, 2025
</div>

<div class="pub-accordion">
  <input id="SIG25a-item1" name="accordion1" type="checkbox" />
  <label for="SIG25a-item1">Abstract</label>
  <div class="pub-abstract">
	We develop an object insertion pipeline and interface that enables iterative editing of illumination-aware composite images. Our pipeline leverages off-the-shelf computer vision methods and differentiable rendering to reconstruct a 3D representation of a given scene. Users can add 3D objects and render them with physically accurate lighting effects.
  </div>

  <input id="SIG25a-item2" name="accordion1" type="checkbox" />
  <label for="SIG25a-item2">Manuscript &amp; more</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="https://yaksoy.github.io/papers/SIG25p-LightAwareCompositing.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/2025Postera-Paper.jpg" title="Paper" /></a></td>
		<td><a href="https://yaksoy.github.io/papers/SIG25p-LightAwareCompositing.jpg" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/2025Postera-Poster.jpg" title="Poster" /></a></td>
		<td><a href="https://youtu.be/QvYROlkmgGI" target="_blank"><img src="https://yaksoy.github.io/images/research/2025Postera-Video.jpg" title="Video" /></a></td>
	</tr></table>
	</div>

  <input id="SIG25a-item3" name="accordion1" type="checkbox" />
  <label for="SIG25a-item3">BibTeX</label>
  <div class="pub-bibtex">
	@INPROCEEDINGS{pengTairaCompositing,<br />
	author={Weikun Peng and Sota Taira and Chris Careaga and Ya\u{g}{\i}z Aksoy},<br />
	title={Interactive Object Insertion with Differentiable Rendering},<br />
	booktitle={SIGGRAPH Posters},<br />
	year={2025},<br />
	}
  </div>
</div>
</td>
</tr>
</table>
		<hr />
		<table>
<tr>
<td class="pub-photocol">
<a href="../NIREditing" target="_blank"><img src="https://yaksoy.github.io/images/research/NIREditing.jpg" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="../NIREditing" target="_blank">Interactive RGB+NIR Photo Editing <i class="fa fa-external-link"></i></a>
</div>
<div class="pub-authors">
	Samuel Antunes Miranda*, Shahrzad Mirzaei*, Mariam Bebawy*, <b>Sebastian Dille</b>, and <b>Yağız Aksoy</b> 
</div>
<div class="pub-venue">
	SIGGRAPH Posters, 2024
</div>

<div class="pub-accordion">
  <input id="SIGp24-item1" name="accordion1" type="checkbox" />
  <label for="SIGp24-item1">Abstract</label>
  <div class="pub-abstract">
	Near-infrared imagery offers great possibilities for creative image editing. Lying outside the visual spectrum, the NIR information can effectively serve as a fourth color channel to common RGB. 
	Compared to the latter, it shows interesting and complementary behavior: its intensity strongly varies with the surface materials in the scene and is less affected by atmospheric perturbations.
	For these reasons, NIR imaging has been a long-standing topic of interest in research and its integration has been proven successful for applications like false coloring, contrast enhancement, image dehazing, and purification of low-light images. 
	Recent developments in smartphone technology have simplified the capturing process, making NIR data readily available for broader use outside the research community. 
	At the same time, existing tools for NIR processing and manipulation are rare and still limited in functionality. 
	With many solutions lacking specialized features, the editing process is inefficient and cumbersome, making them prone to generate suboptimal results. 
	To tackle this issue, we introduce a simple and intuitive photo editing tool that combines RGB and NIR properties, offering functions tailored specifically for the RGB+NIR combination, and granting the user the ability to edit and refine images more creatively.
  </div>

  <input id="SIGp24-item2" name="accordion1" type="checkbox" />
  <label for="SIGp24-item2">Manuscript &amp; more</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="https://yaksoy.github.io/papers/SIGp24-NIREditing.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/NIREditingPaper.jpg" title="Paper" /></a></td>
		<td><a href="https://yaksoy.github.io/papers/SIGp24-NIREditing.jpg" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/NIREditingPosterSmall.jpg" title="Poster" /></a></td>
		<td><a href="https://youtu.be/tNsUQYI7vw8" target="_blank"><img src="https://yaksoy.github.io/images/research/NIREditingVideo.jpg" title="Video" /></a></td>
	</tr></table>
	</div>

  <input id="SIGp24-item3" name="accordion1" type="checkbox" />
  <label for="SIGp24-item3">BibTeX</label>
  <div class="pub-bibtex">
	@INPROCEEDINGS{NIREditing,<br />
	author={Samuel Antunes Miranda and Shahrzad Mirzaei and Mariam Bebawy and Sebastian Dille and Ya\u{g}{\i}z Aksoy},<br />
	title={Interactive RGB+NIR Photo Editing},<br />
	booktitle={SIGGRAPH Posters},<br />
	year={2024},<br />
	}
  </div>
</div>
</td>
</tr>
</table>
		<hr />
		<table>
<tr>
<td class="pub-photocol">
<a href="../parallaxbg" target="_blank"><img src="https://yaksoy.github.io/images/research/parallax.jpg" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="../parallaxbg" target="_blank">Parallax Background Texture Generation <i class="fa fa-external-link"></i></a>
</div>
<div class="pub-authors">
	Brigham Okano, Shao Yu Shen, <b>Sebastian Dille</b>, and <b>Yağız Aksoy</b> 
</div>
<div class="pub-venue">
	SIGGRAPH Posters, 2022
</div>

<div class="pub-accordion">
  <input id="SIG22c-item1" name="accordion1" type="checkbox" />
  <label for="SIG22c-item1">Abstract</label>
  <div class="pub-abstract">
	Art assets for games can be time intensive to produce. 
	Whether it is a full 3D world, or simpler 2D background, creating good looking assets takes time and skills that are not always readily available. 
	Time can be saved by using repeating assets, but visible repetition hurts immersion. 
	Procedural generation techniques can help make repetition less uniform, but do not remove it entirely. 
	Both approaches leave noticeable levels of repetition in the image, and require significant time and skill investments to produce. 
	Video game developers in hobby, game jam, or early prototyping situations may not have access to the required time and skill. 
	We propose a framework to produce layered 2D backgrounds without the need for significant artist time or skill. 
	In our pipeline, the user provides segmented photographic input, instead of creating traditional art, and receives game-ready assets. 
	By utilizing photographs as input, we can achieve both a high level of realism for the resulting background texture as well as a shift from manual work away towards computational run-time which frees up developers for other work.
  </div>

  <input id="SIG22c-item2" name="accordion1" type="checkbox" />
  <label for="SIG22c-item2">Manuscript &amp; more</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="https://yaksoy.github.io/papers/SIG22c-ParallaxBG.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/parallaxPaper.jpg" title="Paper" /></a></td>
		<td><a href="https://yaksoy.github.io/papers/SIG22c-ParallaxBG.jpg" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/parallaxPosterSmall.jpg" title="Poster" /></a></td>
		<td><a href="https://youtu.be/_KWdFy3YipI" target="_blank"><img src="https://yaksoy.github.io/images/research/parallaxVideo.jpg" title="Video" /></a></td>
	</tr></table>
	</div>

  <input id="SIG22c-item3" name="accordion1" type="checkbox" />
  <label for="SIG22c-item3">BibTeX</label>
  <div class="pub-bibtex">
	@INPROCEEDINGS{parallaxBG,<br />
	author={Brigham Okano and Shao Yu Shen and Sebastian Dille and Ya\u{g}{\i}z Aksoy},<br />
	title={Parallax Background Texture Generation},<br />
	booktitle={SIGGRAPH Posters},<br />
	year={2022},<br />
	}
  </div>
</div>
</td>
</tr>
</table>
		<hr />
	  </div>
</div>

<hr />

<table style="text-align:center;"><tr>
		<td><img width="700" src="Theme-SoftSegmentation.jpg" title="Soft Segmentation, Matting, and Keying" /></td>
	</tr></table>

<div class="homepage" style="text-align:justify;">
Yağız Aksoy's PhD work focused on soft segmentation including natural image matting, green screen keying, and color editing. You may have encountered these methods in 
<a href="https://docs.opencv.org/4.x/d4/d40/group__alphamat.html" target="_blank">OpenCV <i class="fa fa-external-link"></i></a> 
or 
<a href="https://www.fxguide.com/fxfeatured/live-action-avos-disney-soft-segmentation-via-copycat-in-nuke/" target="_blank">Nuke CopyCat <i class="fa fa-external-link"></i></a>.
</div>
<div class="page-accordion">
	<input id="Theme5-item1" name="accordion1" type="checkbox" />
	<label for="Theme5-item1">Selected publications <i class="fa fa-angle-down"></i></label>
	  <div>
		<hr />
		<table>
<tr>
<td class="pub-photocol">
<a href="../ifm" target="_blank"><img src="https://yaksoy.github.io/images/research/ifmExt.png" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="../ifm" target="_blank">Designing Effective Inter-Pixel Information Flow for Natural Image Matting <i class="fa fa-external-link"></i></a>
</div>
<div class="pub-authors">
	<b>Yağız Aksoy</b>, Tunç Ozan Aydın and Marc Pollefeys 
</div>
<div class="pub-venue">
	CVPR, 2017 (<i>spotlight</i>)
</div>

<div class="pub-accordion">
  <input id="CVPR17-item1" name="accordion1" type="checkbox" />
  <label for="CVPR17-item1">Abstract</label>
  <div class="pub-abstract">
We present a novel, purely affinity-based natural image matting algorithm. 
Our method relies on carefully defined pixel-to-pixel connections that enable effective use of information available in the image and the trimap. 
We control the information flow from the known-opacity regions into the unknown region, as well as within the unknown region itself, by utilizing multiple definitions of pixel affinities. 
This way we achieve significant improvements on matte quality near challenging regions of the foreground object. 
Among other forms of information flow, we introduce color-mixture flow, which builds upon local linear embedding and effectively encapsulates the relation between different pixel opacities. 
Our resulting novel linear system formulation can be solved in closed-form and is robust against several fundamental challenges in natural matting such as holes and remote intricate structures. 
While our method is primarily designed as a standalone natural matting tool, we show that it can also be used for regularizing mattes obtained by various sampling-based methods. 
Our evaluation using the public alpha matting benchmark suggests a significant performance improvement over the state-of-the-art.
  </div>

  <input id="CVPR17-item2" name="accordion1" type="checkbox" />
  <label for="CVPR17-item2">Manuscript</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="https://yaksoy.github.io/papers/CVPR17-ifm.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/CVPR17Paper.png" title="Paper" /></a></td>
	</tr></table>
	</div>

  <input id="CVPR17-item3" name="accordion1" type="checkbox" />
  <label for="CVPR17-item3">BibTeX</label>
  <div class="pub-bibtex">
	@INPROCEEDINGS{ifm,<br />
	author={Aksoy, Ya\u{g}{\i}z and Ayd{\i}n, Tun\c{c} Ozan and Pollefeys, Marc}, <br />
	booktitle={Proc. CVPR}, <br />
	title={Designing Effective Inter-Pixel Information Flow for Natural Image Matting}, <br />
	year={2017}, <br />
	}
  </div>
</div>
</td>
</tr>
</table>
		<hr />
		<table>
<tr>
<td class="pub-photocol">
<a href="../keying" target="_blank"><img src="https://yaksoy.github.io/images/research/keying.jpg" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="../keying" target="_blank">Interactive High-Quality Green-Screen Keying via Color Unmixing <i class="fa fa-external-link"></i></a>
</div>
<div class="pub-authors">
	<b>Yağız Aksoy</b>, Tunç Ozan Aydın, Marc Pollefeys and Aljoša Smolić
</div>
<div class="pub-venue">
	ACM Transactions on Graphics, 2016
</div>

<div class="pub-accordion">
  <input id="TOG16-item1" name="accordion1" type="checkbox" />
  <label for="TOG16-item1">Abstract</label>
  <div class="pub-abstract">
  Due to the widespread use of compositing in contemporary feature films, green-screen keying has become an essential part of post-production workflows. 
To comply with the ever-increasing quality requirements of the industry, specialized compositing artists spend countless hours using multiple commercial software tools, while eventually having to resort to manual painting because of the many shortcomings of these tools.
Due to the sheer amount of manual labor involved in the process, new green-screen keying approaches that produce better keying results with less user interaction are welcome additions to the compositing artist's arsenal.
We found that --- contrary to the common belief in the research community --- production-quality green-screen keying is still an unresolved problem with its unique challenges. In this paper, we propose a novel green-screen keying method utilizing a new energy minimization-based color unmixing algorithm.
We present comprehensive comparisons with commercial software packages and relevant methods in literature, which show
that the quality of our results is superior to any other currently available green-screen keying solution.
Importantly, using the proposed method, these high-quality results can be generated using only one-tenth of the manual editing time
that a professional compositing artist requires to process the same content having all previous state-of-the-art tools at his disposal.
  </div>

  <input id="TOG16-item2" name="accordion1" type="checkbox" />
  <label for="TOG16-item2">Manuscript &amp; more</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="https://yaksoy.github.io/papers/TOG16-keying.pdf" target="_blank"><img src="https://yaksoy.github.io/images/research/TOG16Paper.png" title="Paper" /></a></td>
		<td><a href="https://yaksoy.github.io/papers/TOG16-supp-currentPractice.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/TOG16Supp.png" title="Supplementary Material" /></a></td>
		<td><a href="https://www.youtube.com/watch?v=u22QPAp5rx0" target="_blank"><img src="https://yaksoy.github.io/images/research/TOG16Video.png" title="Video" /></a></td>
	</tr></table>
	</div>
  
  <input id="TOG16-item3" name="accordion1" type="checkbox" />
  <label for="TOG16-item3">BibTeX</label>
  <div class="pub-bibtex">
	@ARTICLE{keying,<br />
	author={Ya\u{g}{\i}z Aksoy and Tun\c{c} Ozan Ayd{\i}n and Marc Pollefeys and Aljo\v{s}a Smoli\'{c}},<br />
	title={Interactive High-Quality Green-Screen Keying via Color Unmixing},<br />
	journal={ACM Trans. Graph.},<br />
	year={2016},<br />
  volume = {35},<br />
  number = {5},<br />
  pages = {152:1--152:12},<br />
  }
  </div>
</div>
</td>
</tr>
</table>
		<hr />
		<table>
<tr>
<td class="pub-photocol">
<a href="../scs" target="_blank"><img src="https://yaksoy.github.io/images/research/scs.png" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="../scs" target="_blank">Unmixing-Based Soft Color Segmentation for Image Manipulation <i class="fa fa-external-link"></i></a>
</div>
<div class="pub-authors">
	<b>Yağız Aksoy</b>, Tunç Ozan Aydın, Aljoša Smolić and Marc Pollefeys 
</div>
<div class="pub-venue">
	ACM Transactions on Graphics, 2017
</div>

<div class="pub-accordion">
  <input id="TOG17-item1" name="accordion1" type="checkbox" />
  <label for="TOG17-item1">Abstract</label>
  <div class="pub-abstract">
We present a new method for decomposing an image into a set of soft color segments, which are analogous to color layers with alpha channels that have been commonly utilized in modern image manipulation software.
We show that the resulting decomposition serves as an effective intermediate image representation, which can be utilized for performing various, seemingly unrelated image manipulation tasks.
We identify a set of requirements that soft color segmentation methods have to fulfill, and present an in-depth theoretical analysis of prior work.
We propose an energy formulation for producing compact layers of homogeneous colors and a color refinement procedure, as well as a method for automatically estimating a statistical color model from an image.
This results in a novel framework for automatic and high-quality soft color segmentation, which is efficient, parallelizable, and scalable.
We show that our technique is superior in quality compared to previous methods through quantitative analysis as well as visually through an extensive set of examples. 
We demonstrate that our soft color segments can easily be exported to familiar image manipulation software packages and used to produce compelling results for numerous image manipulation applications without forcing the user to learn new tools and workflows.
  </div>

  <input id="TOG17-item2" name="accordion1" type="checkbox" />
  <label for="TOG17-item2">Manuscript &amp; more</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="https://yaksoy.github.io/papers/TOG17-scs.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/TOG17Paper.png" title="Paper" /></a></td>
		<td><a href="https://yaksoy.github.io/papers/TOG17-supp-additionalFigures.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/TOG17Supp.png" title="Additional Figures" /></a></td>
		<td><a href="https://www.youtube.com/watch?v=gf7R_DArdSM" target="_blank"><img src="https://yaksoy.github.io/images/research/TOG17Video.png" title="Video" /></a></td>
	</tr></table>
	</div>

  <input id="TOG17-item3" name="accordion1" type="checkbox" />
  <label for="TOG17-item3">BibTeX</label>
  <div class="pub-bibtex">
	@ARTICLE{scs,<br />
	author={Ya\u{g}{\i}z Aksoy and Tun\c{c} Ozan Ayd{\i}n and Aljo\v{s}a Smoli\'{c} and Marc Pollefeys},<br />
	title={Unmixing-Based Soft Color Segmentation for Image Manipulation},<br />
	journal={ACM Trans. Graph.},<br />
	year={2017},<br />
	pages = {19:1-19:19},<br />
	volume = {36},<br />
	number = {2}<br />
  }
  </div>
</div>
</td>
</tr>
</table>
		<hr />
		<table>
<tr>
<td class="pub-photocol">
<a href="../sss" target="_blank"><img src="https://yaksoy.github.io/images/research/sss.png" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="../sss" target="_blank">Semantic Soft Segmentation <i class="fa fa-external-link"></i></a>
</div>
<div class="pub-authors">
	<b>Yağız Aksoy</b>, Tae-Hyun Oh, Sylvain Paris, Marc Pollefeys and Wojciech Matusik 
</div>
<div class="pub-venue">
	ACM Transactions on Graphics (Proc. SIGGRAPH), 2018
</div>

<div class="pub-accordion">
  <input id="TOG18-item1" name="accordion1" type="checkbox" />
  <label for="TOG18-item1">Abstract</label>
  <div class="pub-abstract">
		Accurate representation of soft transitions between image regions is essential for high-quality image editing and compositing. 
		Current techniques for generating such representations depend heavily on interaction by a skilled visual artist, as creating such accurate object selections is a tedious task.
		In this work, we introduce <i>semantic soft segments</i>, a set of layers that correspond to semantically meaningful regions in an image with accurate soft transitions between different objects.
		We approach this problem from a spectral segmentation angle and propose a graph structure that embeds texture and color features from the image as well as higher-level semantic information generated by a neural network.
		The soft segments are generated via eigendecomposition of the carefully constructed Laplacian matrix fully automatically.
		We demonstrate that otherwise complex image editing tasks can be done with little effort using semantic soft segments.
  </div>

  <input id="TOG18-item2" name="accordion1" type="checkbox" />
  <label for="TOG18-item2">Manuscript &amp; more</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="https://yaksoy.github.io/papers/TOG18-sss.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/TOG18paper.png" title="Paper" /></a></td>
		<td><a href="https://yaksoy.github.io/papers/TOG18-sss-supp.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/TOG18supp.png" title="Supplementary" /></a></td>
		<td><a href="https://youtu.be/QYIQbfnS9jA" target="_blank"><img src="https://yaksoy.github.io/images/research/TOG18Video.png" title="Video" /></a></td>
	</tr></table>
	</div>

  <input id="TOG18-item3" name="accordion1" type="checkbox" />
  <label for="TOG18-item3">BibTeX</label>
  <div class="pub-bibtex">
	@ARTICLE{sss,<br />
	author={Ya\u{g}{\i}z Aksoy and Tae-Hyun Oh and Sylvain Paris and Marc Pollefeys and Wojciech Matusik},<br />
	title={Semantic Soft Segmentation},<br />
	journal={ACM Trans. Graph. (Proc. SIGGRAPH)},<br />
	year={2018},<br />
	pages = {72:1-72:13},<br />
	volume = {37},<br />
	number = {4}<br />
  }
  </div>
</div>
</td>
</tr>
</table>
		<hr />
	  </div>
</div>

<hr />

<table style="text-align:center;"><tr>
		<td><img width="700" src="Theme-Patent.jpg" title="Technology Transfer and Licensing" /></td>
	</tr></table>

<div class="homepage" style="text-align:justify;">
We work with <a href="https://www.sfu.ca/technology-licensing.html" target="_blank">SFU Technology Licensing Office <i class="fa fa-external-link"></i></a> to explore commercial applications of our work and facilitate technology transfers through licensing.
</div>
<div class="page-accordion">
	<input id="Theme6-item1" name="accordion1" type="checkbox" />
	<label for="Theme6-item1">Our patent applications <i class="fa fa-angle-down"></i></label>
	  <div>
		<hr />
		<table>
  <tr>
  <td class="pub-photocol">
  <img src="https://yaksoy.github.io/images/cpdots.jpg" class="pub-photo" />
  </td>
  <td>
  <div class="pub-title">
    Systems and Methods for Image Decomposition
  </div>
  <div class="pub-authors">
    Inventors: <b>Yağız Aksoy</b>, <b>Chris Careaga</b>
    <br />
    Assignee: Simon Fraser University
  </div>
  <div class="pub-venue">
    Patent Application, 2024
    <br />
    Learn more: 
    <a href="/intrinsic/" target="_blank">Intrinsic Decomposition <i class="fa fa-external-link"></i></a>, 
    <a href="/ColorfulShading/" target="_blank">Colorful Intrinsic Decomposition <i class="fa fa-external-link"></i></a>.
  </div>
  </td>
  </tr>
  </table>
		<hr />
		<table>
  <tr>
  <td class="pub-photocol">
  <img src="https://yaksoy.github.io/images/cpdots.jpg" class="pub-photo" />
  </td>
  <td>
  <div class="pub-title">
    Systems and Methods for Image Relighting and Image Compositing
  </div>
  <div class="pub-authors">
    Inventors: <b>Yağız Aksoy</b>, <b>Chris Careaga</b>
    <br />
    Assignee: Simon Fraser University
  </div>
  <div class="pub-venue">
    Patent Application, 2024
    <br />
    Learn more: 
    <a href="/PhysicalRelighting/" target="_blank">Physical Relighting <i class="fa fa-external-link"></i></a>, 
    <a href="/intrinsicCompositing/" target="_blank">Intrinsic Image Compositing <i class="fa fa-external-link"></i></a>.
  </div>
  </td>
  </tr>
  </table>
		<hr />
	  </div>
</div>

<hr />

      </div><!-- /.entry-content -->
    </div><!-- /.entry-wrapper -->
  </article>
</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo" class="entry-wrapper">
    

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
<span>&copy; 2025 Yagiz Aksoy <!--Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a href="http://mademistakes.com/so-simple/" rel="nofollow">So Simple Theme</a>.--></span>
<div class="social-icons">
	<a href="https://scholar.google.com/citations?user=AC445kUAAAAJ" title="Yagiz Aksoy on Google Scholar" target="_blank"><img src="https://yaksoy.github.io/images/logos/scholar.png" alt="Scholar" width="20"></img></a>
	<a href="http://dblp.uni-trier.de/pers/hd/a/Aksoy:Yagiz" title="Yagiz Aksoy on DBLP" target="_blank"><img src="https://yaksoy.github.io/images/logos/dblp.png" alt="DBLP" width="20"></img></a>
	<a href="https://www.researchgate.net/profile/Yagiz_Aksoy" title="Yagiz Aksoy on ResearchGate" target="_blank"><img src="https://yaksoy.github.io/images/logos/rgnew.png" alt="RG" width="20"></img></a>
	<a href="http://orcid.org/0000-0002-1495-0491" title="Yagiz Aksoy on ORCID" target="_blank"><img src="https://yaksoy.github.io/images/logos/orcid.png" alt="ORCID" width="20"></img></a>
	<br><br>
	
	<a href="https://www.youtube.com/channel/UCUmv_OnSSWMCkOESVd6607w" title="Yagiz Aksoy on YouTube" target="_blank"><i class="fa fa-youtube-play fa-2x"></i></a>
	
	
	<a href="http://linkedin.com/in/yaksoy" title="Yagiz Aksoy on LinkedIn" target="_blank"><i class="fa fa-linkedin-square fa-2x"></i></a>
	
	
	
	
	<a href="http://github.com/yaksoy" title="Yagiz Aksoy on Github" target="_blank"><i class="fa fa-github-square fa-2x"></i></a>
	
  
	
  <!--<a href="https://yaksoy.github.io/feed.xml" title="Atom/RSS feed"><i class="fa fa-rss-square fa-2x"></i></a>-->
</div><!-- /.social-icons -->

  </footer>
</div><!-- /.footer-wrapper -->

<script type="text/javascript">
  var BASE_URL = 'https://yaksoy.github.io';
</script>

<!-- Include Latex style math -->
<!-- https://stackoverflow.com/questions/10987992/using-mathjax-with-jekyll -->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="https://yaksoy.github.io/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="https://yaksoy.github.io/assets/js/scripts.min.js"></script>



<!-- Start of StatCounter Code for Default Guide -->
<script type="text/javascript">
var sc_project=11195487; 
var sc_invisible=1; 
var sc_security="112d9b46"; 
var scJsHost = (("https:" == document.location.protocol) ?
"https://secure." : "http://www.");
document.write("<sc"+"ript type='text/javascript' src='" +
scJsHost+
"statcounter.com/counter/counter.js'></"+"script>");
</script>
<noscript><div class="statcounter"><a title="shopify
analytics" href="http://statcounter.com/shopify/"
target="_blank"><img class="statcounter"
src="//c.statcounter.com/11195487/0/112d9b46/1/"
alt="shopify analytics"></a></div></noscript>
<!-- End of StatCounter Code for Default Guide -->

</body>
</html>
