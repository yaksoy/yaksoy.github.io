<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Publications &#8211; Yağız Aksoy</title>
<meta name="description" content="Publications">



<!-- Twitter Cards -->
<meta name="twitter:title" content="Publications">
<meta name="twitter:description" content="Publications">
<meta name="twitter:site" content="@yagizaksoy">
<meta name="twitter:creator" content="@yagizaksoy">

<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yaksoy.github.io/images/jordan.png">

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Publications">
<meta property="og:description" content="Publications">
<meta property="og:url" content="http://yaksoy.github.io/research/">
<meta property="og:site_name" content="Yağız Aksoy">

<!-- Webmaster Tools verfication -->
<meta name="google-site-verification" content="googleb0479c04a25255c3">



<link rel="canonical" href="http://yaksoy.github.io/research/">
<link href="http://yaksoy.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Yağız Aksoy Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://yaksoy.github.io/assets/css/main.css">
<!-- Webfonts -->
<script src="//use.edgefonts.net/source-sans-pro:n2,i2,n3,i3,n4,i4,n6,i6,n7,i7,n9,i9;source-code-pro:n4,n7;volkhov.js"></script>

<meta http-equiv="cleartype" content="on">

<!-- HTML5 Shiv and Media Query Support -->
<!--[if lt IE 9]>
  <script src="http://yaksoy.github.io/assets/js/vendor/html5shiv.min.js"></script>
  <script src="http://yaksoy.github.io/assets/js/vendor/respond.min.js"></script>
<![endif]-->

<!-- Modernizr -->
<script src="http://yaksoy.github.io/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://yaksoy.github.io/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://yaksoy.github.io/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://yaksoy.github.io/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://yaksoy.github.io/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://yaksoy.github.io/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://yaksoy.github.io/images/apple-touch-icon-144x144-precomposed.png">

</head>

<body id="page">

<div class="navigation-wrapper">
	<nav role="navigation" id="site-nav" class="animated drop">
	    <ul>
      
		    
		    <li><a href="http://yaksoy.github.io/" >Home</a></li>
		  
		    
		    <li><a href="http://yaksoy.github.io/group/" >Group</a></li>
		  
		    
		    <li><a href="http://yaksoy.github.io/research/" >Publications</a></li>
		  
		    
		    <li><a href="http://yaksoy.github.io/code/" >Code & Data</a></li>
		  
		    
		    <li><a href="http://yaksoy.github.io/teaching/" >Teaching</a></li>
		  
		    
		    <li><a href="http://yaksoy.github.io/contact/" >Contact</a></li>
		  
	    </ul>
	</nav>
</div><!-- /.navigation-wrapper -->

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->

<header class="masthead">
	<div class="wrap">
      
  		<a href="http://yaksoy.github.io/" class="site-logo" rel="home" title="Yağız Aksoy"><img src="http://yaksoy.github.io/images/jordan.png" width="200" height="200" alt="Yağız Aksoy logo" class="animated fadeInDown"></a>
      
      <h1 class="site-title animated fadeIn"><a href="http://yaksoy.github.io/">Yağız Aksoy</a></h1>
		<h2 class="site-description animated fadeIn" itemprop="description">Computational Photography Lab @ SFU</h2>
	</div>
</header><!-- /.masthead -->

<div class="js-menu-screen menu-screen"></div>


<div id="main" role="main">
  <article class="entry">
    
    <div class="entry-wrapper">
      <header class="entry-header">
        <h1 class="entry-title" style="margin-top:-5px;">Publications</h1>
      </header>
      <div class="entry-content">
        <h3 id="major-venues">Major venues</h3>

<hr />

<table>
	<tr>
	<td class="pub-photocol">
	<a href="../intrinsicFlash" target="_blank"><img src="http://yaksoy.github.io/images/research/intrinsicFlash.jpg" class="pub-photo" /></a>
	</td>
	<td>
	<div class="pub-title">
		<a href="../intrinsicFlash" target="_blank">Computational Flash Photography through Intrinsics <i class="fa fa-external-link"></i></a>
	</div>
	<div class="pub-authors">
		<b>Sepideh Sarajian Maralan</b>, <b>Chris Careaga</b>, and <b>Yağız Aksoy</b> 
	</div>
	<div class="pub-venue">
		CVPR, 2023
	</div>
	
	<div class="accordion">
	  <input id="CVPR23b-item1" name="accordion1" type="checkbox" />
	  <label for="CVPR23b-item1">Abstract</label>
	  <div class="pub-abstract">
		Flash is an essential tool as it often serves as the sole controllable light source in everyday photography. 
		However, the use of flash is a binary decision at the time a photograph is captured with limited control over its characteristics such as strength or color. 
		In this work, we study the computational control of the flash light in photographs taken with or without flash. 
		We present a physically motivated intrinsic formulation for flash photograph formation and develop flash decomposition and generation methods for flash and no-flash photographs, respectively. 
		We demonstrate that our intrinsic formulation outperforms alternatives in the literature and allows us to computationally control flash in in-the-wild images.
	</div>
	
	  <input id="CVPR23b-item2" name="accordion1" type="checkbox" />
	  <label for="CVPR23b-item2">Manuscript &amp; more</label>
	  <div class="pub-photolink">
	  
		<table><tr>
			<td><a href="http://yaksoy.github.io/papers/CVPR23-IntrinsicFlash.pdf" target="_blank"><img width="200" src="http://yaksoy.github.io/images/research/CVPR23bPaper.jpg" title="Paper" /></a></td>
			<td><a href="http://yaksoy.github.io/papers/CVPR23-IntrinsicFlash-Dataset.pdf" target="_blank"><img width="200" src="http://yaksoy.github.io/images/research/CVPR23bSupp1.jpg" title="Dataset" /></a></td>
			<td><a href="http://yaksoy.github.io/papers/CVPR23-IntrinsicFlash-SuppResults.pdf" target="_blank"><img width="200" src="http://yaksoy.github.io/images/research/CVPR23bSupp2.jpg" title="Supplementary" /></a></td>
		</tr></table>
		</div>
	
	  <input id="CVPR23b-item3" name="accordion1" type="checkbox" />
	  <label for="CVPR23b-item3">BibTeX</label>
	  <div class="pub-bibtex">
	@INPROCEEDINGS{Maralan2023Flash,<br />
		author={Sepideh Sarajian Maralan and Chris Careaga and Ya\u{g}{\i}z Aksoy},<br />
		title={Computational Flash Photography through Intrinsics},<br />
		journal={Proc. CVPR},<br />
		year={2023},<br />
		} 
	  </div>
	</div>
	</td>
	</tr>
	</table>

<hr />

<table>
<tr>
<td class="pub-photocol">
<a href="../highresdepth" target="_blank"><img src="http://yaksoy.github.io/images/research/highresdepth.jpg" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="../highresdepth" target="_blank">Boosting Monocular Depth Estimation Models to High Resolution via Content Adaptive Multi Resolution Merging <i class="fa fa-external-link"></i></a>
</div>
<div class="pub-authors">
	<i><b>S. Mahdi H. Miangoleh*</b></i>, <i><b>Sebastian Dille*</b></i>, Long Mai, Sylvain Paris, and <b>Yağız Aksoy</b> 
</div>
<div class="pub-venue">
	CVPR, 2021
</div>

<div class="accordion">
  <input id="CVPR21-item1" name="accordion1" type="checkbox" />
  <label for="CVPR21-item1">Abstract</label>
  <div class="pub-abstract">
	Neural networks have shown great abilities in estimating depth from a single image. 
	However, the inferred depth maps are well below one-megapixel resolution and often lack fine-grained details, which limits their practicality. 
	Our method builds on our analysis on how the input resolution and the scene structure affects depth estimation performance. 
	We demonstrate that there is a trade-off between a consistent scene structure and the high-frequency details, and merge low- and high-resolution estimations to take advantage of this duality using a simple depth merging network. 
	We present a double estimation method that improves the whole-image depth estimation and a patch selection method that adds local details to the final result. 
	We demonstrate that by merging estimations at different resolutions with changing context, we can generate multi-megapixel depth maps with a high level of detail using a pre-trained model.
</div>

  <input id="CVPR21-item2" name="accordion1" type="checkbox" />
  <label for="CVPR21-item2">Manuscript &amp; more</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="http://yaksoy.github.io/papers/CVPR21-HighResDepth.pdf" target="_blank"><img width="200" src="http://yaksoy.github.io/images/research/CVPR21Paper.jpg" title="Paper" /></a></td>
		<td><a href="http://yaksoy.github.io/papers/CVPR21-HighResDepth-Supp.pdf" target="_blank"><img width="200" src="http://yaksoy.github.io/images/research/CVPR21Supp.jpg" title="Paper" /></a></td>
		<td><a href="https://youtu.be/lDeI17pHlqo" target="_blank"><img src="http://yaksoy.github.io/images/research/CVPR21Video.jpg" title="Video" /></a></td>
	</tr></table>
	</div>

  <input id="CVPR21-item3" name="accordion1" type="checkbox" />
  <label for="CVPR21-item3">BibTeX</label>
  <div class="pub-bibtex">
@INPROCEEDINGS{Miangoleh2021Boosting,<br />
	author={S. Mahdi H. Miangoleh and Sebastian Dille and Long Mai and Sylvain Paris and Ya\u{g}{\i}z Aksoy},<br />
	title={Boosting Monocular Depth Estimation Models to High-Resolution via Content-Adaptive Multi-Resolution Merging},<br />
	journal={Proc. CVPR},<br />
	year={2021},<br />
	} 
  </div>
</div>
</td>
</tr>
</table>

<hr />

<table>
<tr>
<td class="pub-photocol">
<a href="http://www-sop.inria.fr/reves/Basilic/2021/TAAPDD21/" target="_blank"><img src="http://yaksoy.github.io/images/research/vbr.png" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="http://www-sop.inria.fr/reves/Basilic/2021/TAAPDD21/" target="_blank">Video-Based Rendering of Dynamic Stationary Environments from Unsynchronized Inputs
		<i class="fa fa-external-link"></i></a>
</div>
<div class="pub-authors">
	Theo Thonat, <b>Yağız Aksoy</b>, Miika Aittala, Sylvain Paris, Frédo Durand, and George Drettakis
</div>
<div class="pub-venue">
	Computer Graphics Forum (Proc. EGSR), 2021
</div>

<div class="accordion">
  <input id="CGF21-item1" name="accordion1" type="checkbox" />
  <label for="CGF21-item1">Abstract</label>
  <div class="pub-abstract">
	Image-Based Rendering allows users to easily capture a scene using a single camera and then navigate freely with realistic results. However, the resulting renderings are completely static, and dynamic effects – such as fire, waterfalls or small waves – cannot be reproduced. We tackle the challenging problem of enabling free-viewpoint navigation including such stationary dynamic effects, but still maintaining the simplicity of casual capture. Using a single camera – instead of previous complex synchronized multi-camera setups – means that we have unsynchronized videos of the dynamic effect from multiple views, making it hard to blend them when synthesizing novel views. We present a solution that allows smooth free-viewpoint video-based rendering (VBR) of such scenes using temporal Laplacian pyramid decomposition video, enabling spatio-temporal blending. For effects such as fire and waterfalls, that are semi-transparent and occupy 3D space, we first estimate their spatial volume. This allows us to create per-video geometries and alpha-matte videos that we can blend using our frequency-dependent method. We also extend Laplacian blending to the temporal dimension to remove additional temporal seams. We show results on scenes containing fire, waterfalls or rippling waves at the seaside, bringing these scenes to life.
  </div>

  <input id="CGF21-item2" name="accordion1" type="checkbox" />
  <label for="CGF21-item2">Manuscript &amp; more</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="http://www-sop.inria.fr/reves/Basilic/2021/TAAPDD21/ThonatVideoBasedRenderingUnsycnhronizedInputsCGF_EGSR21_AuthorsVersion.pdf" target="_blank"><img width="200" src="http://yaksoy.github.io/images/research/CGF21paper.png" title="Paper" /></a></td>
		<td><a href="http://www-sop.inria.fr/reves/Basilic/2021/TAAPDD21/ThonatVideoBasedRenderingSupplemental.pdf" target="_blank"><img width="200" src="http://yaksoy.github.io/images/research/CGF21supp.png" title="Supplementary" /></a></td>
	</tr></table>
	</div>

  <input id="CGF21-item3" name="accordion1" type="checkbox" />
  <label for="CGF21-item3">BibTeX</label>
  <div class="pub-bibtex">
	@ARTICLE{Thonat2021,<br />
	author={Thonat, Theo and Aksoy, Ya\u{g}{\i}z and Aittala, Miika and Paris, Sylvain and Durand, Fr\'edo and Drettakis, George},<br />
	title={Video-Based Rendering of Dynamic Stationary Environments from Unsynchronized Inputs},<br />
	journal={Computer Graphics Forum (Proc. EGSR)},<br />
	year={2021},<br />
	volume = {40},<br />
	number = {4}<br />
  }
  </div>
</div>
</td>
</tr>
</table>

<hr />

<table>
<tr>
<td class="pub-photocol">
<a href="../samplenet" target="_blank"><img src="http://yaksoy.github.io/images/research/samplenet.jpg" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="../samplenet" target="_blank">Learning-based Sampling for Natural Image Matting <i class="fa fa-external-link"></i></a>
</div>
<div class="pub-authors">
	Jingwei Tang, <b>Yağız Aksoy</b>, Cengiz Öztireli, Markus Gross and Tunç Ozan Aydın 
</div>
<div class="pub-venue">
	CVPR, 2019
</div>

<div class="accordion">
  <input id="CVPR19-item1" name="accordion1" type="checkbox" />
  <label for="CVPR19-item1">Abstract</label>
  <div class="pub-abstract">
The goal of natural image matting is the estimation of opacities of a user-defined foreground object that is essential in creating realistic composite imagery. 
Natural matting is a challenging process due to the high number of unknowns in the mathematical modeling of the problem, namely the opacities as well as the foreground and background layer colors, while the original image serves as the single observation. 
In this paper, we propose the estimation of the layer colors through the use of deep neural networks prior to the opacity estimation. 
The layer color estimation is a better match for the capabilities of neural networks, and the availability of these colors substantially increase the performance of opacity estimation due to the reduced number of unknowns in the compositing equation. 
A prominent approach to matting in parallel to ours is called sampling-based matting, which involves gathering
color samples from known-opacity regions to predict the layer colors. 
Our approach outperforms not only the previous hand-crafted sampling algorithms, but also current data-driven methods. 
We hence classify our method as a hybrid sampling- and learning-based approach to matting, and demonstrate the effectiveness of our approach through detailed ablation studies using alternative network architectures.
</div>

  <input id="CVPR19-item2" name="accordion1" type="checkbox" />
  <label for="CVPR19-item2">Manuscript</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="http://yaksoy.github.io/papers/CVPR19-samplenet.pdf" target="_blank"><img width="200" src="http://yaksoy.github.io/images/research/CVPR19Paper.png" title="Paper" /></a></td>
	</tr></table>
	</div>

  <input id="CVPR19-item3" name="accordion1" type="checkbox" />
  <label for="CVPR19-item3">BibTeX</label>
  <div class="pub-bibtex">
	@INPROCEEDINGS{samplenet,<br />
	author={Tang, Jingwei and Aksoy, Ya\u{g}{\i}z and \"Oztireli, Cengiz and Gross, Markus and Ayd{\i}n, Tun\c{c} Ozan}, <br />
	booktitle={Proc. CVPR}, <br />
	title={Learning-based Sampling for Natural Image Matting}, <br />
	year={2019}, <br />
	}
  </div>
</div>
</td>
</tr>
</table>

<hr />

<table>
<tr>
<td class="pub-photocol">
<a href="../sss" target="_blank"><img src="http://yaksoy.github.io/images/research/sss.png" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="../sss" target="_blank">Semantic Soft Segmentation <i class="fa fa-external-link"></i></a>
</div>
<div class="pub-authors">
	<b>Yağız Aksoy</b>, Tae-Hyun Oh, Sylvain Paris, Marc Pollefeys and Wojciech Matusik 
</div>
<div class="pub-venue">
	ACM Transactions on Graphics (Proc. SIGGRAPH), 2018
</div>

<div class="accordion">
  <input id="TOG18-item1" name="accordion1" type="checkbox" />
  <label for="TOG18-item1">Abstract</label>
  <div class="pub-abstract">
		Accurate representation of soft transitions between image regions is essential for high-quality image editing and compositing. 
		Current techniques for generating such representations depend heavily on interaction by a skilled visual artist, as creating such accurate object selections is a tedious task.
		In this work, we introduce <i>semantic soft segments</i>, a set of layers that correspond to semantically meaningful regions in an image with accurate soft transitions between different objects.
		We approach this problem from a spectral segmentation angle and propose a graph structure that embeds texture and color features from the image as well as higher-level semantic information generated by a neural network.
		The soft segments are generated via eigendecomposition of the carefully constructed Laplacian matrix fully automatically.
		We demonstrate that otherwise complex image editing tasks can be done with little effort using semantic soft segments.
  </div>

  <input id="TOG18-item2" name="accordion1" type="checkbox" />
  <label for="TOG18-item2">Manuscript &amp; more</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="http://yaksoy.github.io/papers/TOG18-sss.pdf" target="_blank"><img width="200" src="http://yaksoy.github.io/images/research/TOG18paper.png" title="Paper" /></a></td>
		<td><a href="http://yaksoy.github.io/papers/TOG18-sss-supp.pdf" target="_blank"><img width="200" src="http://yaksoy.github.io/images/research/TOG18supp.png" title="Supplementary" /></a></td>
		<td><a href="https://youtu.be/QYIQbfnS9jA" target="_blank"><img src="http://yaksoy.github.io/images/research/TOG18Video.png" title="Video" /></a></td>
	</tr></table>
	</div>

  <input id="TOG18-item3" name="accordion1" type="checkbox" />
  <label for="TOG18-item3">BibTeX</label>
  <div class="pub-bibtex">
	@ARTICLE{sss,<br />
	author={Ya\u{g}{\i}z Aksoy and Tae-Hyun Oh and Sylvain Paris and Marc Pollefeys and Wojciech Matusik},<br />
	title={Semantic Soft Segmentation},<br />
	journal={ACM Trans. Graph. (Proc. SIGGRAPH)},<br />
	year={2018},<br />
	pages = {72:1-72:13},<br />
	volume = {37},<br />
	number = {4}<br />
  }
  </div>
</div>
</td>
</tr>
</table>

<hr />

<table>
<tr>
<td class="pub-photocol">
<a href="../flashambient" target="_blank"><img src="http://yaksoy.github.io/images/research/fai.png" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="../flashambient" target="_blank">A Dataset of Flash and Ambient Illumination Pairs from the Crowd <i class="fa fa-external-link"></i></a>
</div>
<div class="pub-authors">
	<b>Yağız Aksoy</b>, Changil Kim, Petr Kellnhofer, Sylvain Paris, Mohamed Elgharib, Marc Pollefeys and Wojciech Matusik 
</div>
<div class="pub-venue">
	ECCV, 2018
</div>

<div class="accordion">
  <input id="ECCV18-item1" name="accordion1" type="checkbox" />
  <label for="ECCV18-item1">Abstract</label>
  <div class="pub-abstract">
Illumination is a critical element of photography and is essential for many computer vision tasks. 
Flash light is unique in the sense that it is a widely available tool for easily manipulating the scene illumination. 
We present a dataset of thousands of ambient and flash illumination pairs to enable studying flash photography and other applications that can benefit from having separate illuminations. 
Different than the typical use of crowdsourcing in generating computer vision datasets, we make use of the crowd to directly take the photographs that make up our dataset. 
As a result, our dataset covers a wide variety of scenes captured by many casual photographers. 
We detail the advantages and challenges of our approach to crowdsourcing as well as the computational effort to generate completely separate flash illuminations from the ambient light in an uncontrolled setup. 
We present a brief examination of illumination decomposition, a challenging and underconstrained problem in flash photography, to demonstrate the use of our dataset in a data-driven approach.
  </div>

  <input id="ECCV18-item2" name="accordion1" type="checkbox" />
  <label for="ECCV18-item2">Manuscript</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="http://yaksoy.github.io/papers/ECCV18-flashambient.pdf" target="_blank"><img width="200" src="http://yaksoy.github.io/images/research/ECCV18paper.png" title="Paper" /></a></td>
		<td><a href="http://yaksoy.github.io/papers/ECCV18-flashambient-supp.zip" target="_blank"><img width="200" src="http://yaksoy.github.io/images/research/ECCV18supp.png" title="Paper" /></a></td>
	</tr></table>
	</div>

  <input id="ECCV18-item3" name="accordion1" type="checkbox" />
  <label for="ECCV18-item3">BibTeX</label>
  <div class="pub-bibtex">
	@INPROCEEDINGS{flashambient,<br />
	author={Ya\u{g}{\i}z Aksoy and Changil Kim and Petr Kellnhofer and Sylvain Paris and Mohamed Elgharib and Marc Pollefeys and Wojciech Matusik}, <br />
	booktitle={Proc. ECCV}, <br />
	title={A Dataset of Flash and Ambient Illumination Pairs from the Crowd}, <br />
	year={2018}, <br />
	}
  </div>
</div>
</td>
</tr>
</table>

<hr />

<table>
<tr>
<td class="pub-photocol">
  <a href="../crowdroto" target="_blank"><img src="http://yaksoy.github.io/images/research/crowdroto2.jpg" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="../crowdroto" target="_blank">Crowd-Guided Ensembles: How Can We Choreograph Crowd Workers for Video Segmentation? <i class="fa fa-external-link"></i></a>
</div>
<div class="pub-authors">
	Alexandre Kaspar, Geneviève Patterson, Changil Kim, <b>Yağız Aksoy</b>, Wojciech Matusik and Mohamed Elgharib 
</div>
<div class="pub-venue">
	ACM CHI Conference on Human Factors in Computing Systems, 2018
</div>

<div class="accordion">
  <input id="CHI18-item1" name="accordion1" type="checkbox" />
  <label for="CHI18-item1">Abstract</label>
  <div class="pub-abstract">
In this work, we propose two ensemble methods leveraging a crowd workforce to improve video annotation, with a focus on video object segmentation.
Their shared principle is that while individual candidate results may likely be insufficient, they often complement each other so that they can be combined into something better than any of the individual results - the very spirit of collaborative working.
For one, we extend a standard polygon-drawing interface to allow workers to annotate negative space, and combine the work of multiple workers instead of relying on a single best one as commonly done in crowdsourced image segmentation.
For the other, we present a method to combine multiple automatic propagation algorithms with the help of the crowd.
Such combination requires an understanding of where the algorithms fail, which we gather using a novel coarse scribble video annotation task.
We evaluate our ensemble methods, discuss our design choices for them, and make our web-based crowdsourcing tools and results publicly available.
  </div>

  <input id="CHI18-item2" name="accordion1" type="checkbox" />
  <label for="CHI18-item2">Manuscript &amp; more</label>
  <div class="pub-photolink">
  
	<table><tr>
    <td><a href="http://yaksoy.github.io/papers/CHI18-ensembles.pdf" target="_blank"><img width="200" src="http://yaksoy.github.io/images/research/CHI18paper.png" title="Paper" /></a></td>
    <td><a href="http://yaksoy.github.io/papers/CHI18-supp-additionalResults.pdf" target="_blank"><img width="200" src="http://yaksoy.github.io/images/research/CHI18supp.png" title="Additional Figures" /></a></td>
	</tr></table>
	</div>

  <input id="CHI18-item3" name="accordion1" type="checkbox" />
  <label for="CHI18-item3">BibTeX</label>
  <div class="pub-bibtex">
	@INPROCEEDINGS{crowdensembles,<br />
	author={Alexandre Kaspar and Genevi\`eve Patterson and Changil Kim and Ya\u{g}{\i}z Aksoy and Wojciech Matusik and Mohamed Elgharib},<br />
	title={Crowd-Guided Ensembles: How Can We Choreograph Crowd Workers for Video Segmentation?},<br />
	booktitle={Proc. ACM CHI},<br />
	year={2018},<br />
  }
  </div>
</div>
</td>
</tr>
</table>

<hr />

<table>
<tr>
<td class="pub-photocol">
<a href="../ifm" target="_blank"><img src="http://yaksoy.github.io/images/research/ifmExt.png" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="../ifm" target="_blank">Designing Effective Inter-Pixel Information Flow for Natural Image Matting <i class="fa fa-external-link"></i></a>
</div>
<div class="pub-authors">
	<b>Yağız Aksoy</b>, Tunç Ozan Aydın and Marc Pollefeys 
</div>
<div class="pub-venue">
	CVPR, 2017 (<i>spotlight</i>)
</div>

<div class="accordion">
  <input id="CVPR17-item1" name="accordion1" type="checkbox" />
  <label for="CVPR17-item1">Abstract</label>
  <div class="pub-abstract">
We present a novel, purely affinity-based natural image matting algorithm. 
Our method relies on carefully defined pixel-to-pixel connections that enable effective use of information available in the image and the trimap. 
We control the information flow from the known-opacity regions into the unknown region, as well as within the unknown region itself, by utilizing multiple definitions of pixel affinities. 
This way we achieve significant improvements on matte quality near challenging regions of the foreground object. 
Among other forms of information flow, we introduce color-mixture flow, which builds upon local linear embedding and effectively encapsulates the relation between different pixel opacities. 
Our resulting novel linear system formulation can be solved in closed-form and is robust against several fundamental challenges in natural matting such as holes and remote intricate structures. 
While our method is primarily designed as a standalone natural matting tool, we show that it can also be used for regularizing mattes obtained by various sampling-based methods. 
Our evaluation using the public alpha matting benchmark suggests a significant performance improvement over the state-of-the-art.
  </div>

  <input id="CVPR17-item2" name="accordion1" type="checkbox" />
  <label for="CVPR17-item2">Manuscript</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="http://yaksoy.github.io/papers/CVPR17-ifm.pdf" target="_blank"><img width="200" src="http://yaksoy.github.io/images/research/CVPR17Paper.png" title="Paper" /></a></td>
	</tr></table>
	</div>

  <input id="CVPR17-item3" name="accordion1" type="checkbox" />
  <label for="CVPR17-item3">BibTeX</label>
  <div class="pub-bibtex">
	@INPROCEEDINGS{ifm,<br />
	author={Aksoy, Ya\u{g}{\i}z and Ayd{\i}n, Tun\c{c} Ozan and Pollefeys, Marc}, <br />
	booktitle={Proc. CVPR}, <br />
	title={Designing Effective Inter-Pixel Information Flow for Natural Image Matting}, <br />
	year={2017}, <br />
	}
  </div>
</div>
</td>
</tr>
</table>

<hr />

<table>
<tr>
<td class="pub-photocol">
<a href="../scs" target="_blank"><img src="http://yaksoy.github.io/images/research/scs.png" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="../scs" target="_blank">Unmixing-Based Soft Color Segmentation for Image Manipulation <i class="fa fa-external-link"></i></a>
</div>
<div class="pub-authors">
	<b>Yağız Aksoy</b>, Tunç Ozan Aydın, Aljoša Smolić and Marc Pollefeys 
</div>
<div class="pub-venue">
	ACM Transactions on Graphics, 2017
</div>

<div class="accordion">
  <input id="TOG17-item1" name="accordion1" type="checkbox" />
  <label for="TOG17-item1">Abstract</label>
  <div class="pub-abstract">
We present a new method for decomposing an image into a set of soft color segments, which are analogous to color layers with alpha channels that have been commonly utilized in modern image manipulation software.
We show that the resulting decomposition serves as an effective intermediate image representation, which can be utilized for performing various, seemingly unrelated image manipulation tasks.
We identify a set of requirements that soft color segmentation methods have to fulfill, and present an in-depth theoretical analysis of prior work.
We propose an energy formulation for producing compact layers of homogeneous colors and a color refinement procedure, as well as a method for automatically estimating a statistical color model from an image.
This results in a novel framework for automatic and high-quality soft color segmentation, which is efficient, parallelizable, and scalable.
We show that our technique is superior in quality compared to previous methods through quantitative analysis as well as visually through an extensive set of examples. 
We demonstrate that our soft color segments can easily be exported to familiar image manipulation software packages and used to produce compelling results for numerous image manipulation applications without forcing the user to learn new tools and workflows.
  </div>

  <input id="TOG17-item2" name="accordion1" type="checkbox" />
  <label for="TOG17-item2">Manuscript &amp; more</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="http://yaksoy.github.io/papers/TOG17-scs.pdf" target="_blank"><img width="200" src="http://yaksoy.github.io/images/research/TOG17Paper.png" title="Paper" /></a></td>
		<td><a href="http://yaksoy.github.io/papers/TOG17-supp-additionalFigures.pdf" target="_blank"><img width="200" src="http://yaksoy.github.io/images/research/TOG17Supp.png" title="Additional Figures" /></a></td>
		<td><a href="https://www.youtube.com/watch?v=gf7R_DArdSM" target="_blank"><img src="http://yaksoy.github.io/images/research/TOG17Video.png" title="Video" /></a></td>
	</tr></table>
	</div>

  <input id="TOG17-item3" name="accordion1" type="checkbox" />
  <label for="TOG17-item3">BibTeX</label>
  <div class="pub-bibtex">
	@ARTICLE{scs,<br />
	author={Ya\u{g}{\i}z Aksoy and Tun\c{c} Ozan Ayd{\i}n and Aljo\v{s}a Smoli\'{c} and Marc Pollefeys},<br />
	title={Unmixing-Based Soft Color Segmentation for Image Manipulation},<br />
	journal={ACM Trans. Graph.},<br />
	year={2017},<br />
	pages = {19:1-19:19},<br />
	volume = {36},<br />
	number = {2}<br />
  }
  </div>
</div>
</td>
</tr>
</table>

<hr />

<table>
<tr>
<td class="pub-photocol">
<a href="../keying" target="_blank"><img src="http://yaksoy.github.io/images/research/keying.jpg" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="../keying" target="_blank">Interactive High-Quality Green-Screen Keying via Color Unmixing <i class="fa fa-external-link"></i></a>
</div>
<div class="pub-authors">
	<b>Yağız Aksoy</b>, Tunç Ozan Aydın, Marc Pollefeys and Aljoša Smolić
</div>
<div class="pub-venue">
	ACM Transactions on Graphics, 2016
</div>

<div class="accordion">
  <input id="TOG16-item1" name="accordion1" type="checkbox" />
  <label for="TOG16-item1">Abstract</label>
  <div class="pub-abstract">
  Due to the widespread use of compositing in contemporary feature films, green-screen keying has become an essential part of post-production workflows. 
To comply with the ever-increasing quality requirements of the industry, specialized compositing artists spend countless hours using multiple commercial software tools, while eventually having to resort to manual painting because of the many shortcomings of these tools.
Due to the sheer amount of manual labor involved in the process, new green-screen keying approaches that produce better keying results with less user interaction are welcome additions to the compositing artist's arsenal.
We found that --- contrary to the common belief in the research community --- production-quality green-screen keying is still an unresolved problem with its unique challenges. In this paper, we propose a novel green-screen keying method utilizing a new energy minimization-based color unmixing algorithm.
We present comprehensive comparisons with commercial software packages and relevant methods in literature, which show
that the quality of our results is superior to any other currently available green-screen keying solution.
Importantly, using the proposed method, these high-quality results can be generated using only one-tenth of the manual editing time
that a professional compositing artist requires to process the same content having all previous state-of-the-art tools at his disposal.
  </div>

  <input id="TOG16-item2" name="accordion1" type="checkbox" />
  <label for="TOG16-item2">Manuscript &amp; more</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="http://yaksoy.github.io/papers/TOG16-keying.pdf" target="_blank"><img src="http://yaksoy.github.io/images/research/TOG16Paper.png" title="Paper" /></a></td>
		<td><a href="http://yaksoy.github.io/papers/TOG16-supp-currentPractice.pdf" target="_blank"><img width="200" src="http://yaksoy.github.io/images/research/TOG16Supp.png" title="Supplementary Material" /></a></td>
		<td><a href="https://www.youtube.com/watch?v=u22QPAp5rx0" target="_blank"><img src="http://yaksoy.github.io/images/research/TOG16Video.png" title="Video" /></a></td>
	</tr></table>
	</div>
  
  <input id="TOG16-item3" name="accordion1" type="checkbox" />
  <label for="TOG16-item3">BibTeX</label>
  <div class="pub-bibtex">
	@ARTICLE{keying,<br />
	author={Ya\u{g}{\i}z Aksoy and Tun\c{c} Ozan Ayd{\i}n and Marc Pollefeys and Aljo\v{s}a Smoli\'{c}},<br />
	title={Interactive High-Quality Green-Screen Keying via Color Unmixing},<br />
	journal={ACM Trans. Graph.},<br />
	year={2016},<br />
  volume = {35},<br />
  number = {5},<br />
  pages = {152:1--152:12},<br />
  }
  </div>
</div>
</td>
</tr>
</table>

<hr />

<h4>Theses</h4>

<hr />

<table>
<tr>
<td class="pub-photocol">
<a href="http://yaksoy.github.io/cfp-msc/" target="_blank"><img src="http://yaksoy.github.io/images/research/sepidehmsc.jpg" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="http://yaksoy.github.io/cfp-msc/" target="_blank">Computational Flash Photography <i class="fa fa-external-link"></i></a>
</div>
<div class="pub-authors">
	<b>Sepideh Sarajian Maralan</b>
</div>
<div class="pub-venue">
	MSc Thesis, Simon Fraser University, 2022
</div>

<div class="accordion">
	<input id="SFU22b-item1" name="accordion1" type="checkbox" />
	<label for="SFU22b-item1">Abstract</label>
	<div class="pub-abstract">
		The majority of common cameras have an integrated flash that improves lighting in a variety of situations, particularly in low-light environments. 
		Before capturing an image, the photographer must make a decision regarding the usage of flash. 
		However, flash strength cannot be adjusted once it has been utilised in an image.
		In this work, we target two application scenarios in computational flash photography: decomposition of a flash photograph into its illumination components and generating the flash illumination from a given single no-flash photograph. 
		Two distinct approaches based on image-to-image transfer and intrinsic decomposition with the use of convolutional neural networks are employed to address these tasks. 
		An additional network boosts and upscales the estimated results to generate the final illuminations. 
		Key advantages of our approach include the preparation of a large flash/no-flash dataset and presenting models based on state-of-the-art methods to address subtasks specific to our problem.
  </div>
  
  <input id="SFU22b-item2" name="accordion1" type="checkbox" />
  <label for="SFU22b-item2">Thesis and Presentation</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="https://www.dropbox.com/s/lksa3a2m4w1lllp/MaralanSFU22MSc.pdf?dl=0" target="_blank"><img src="http://yaksoy.github.io/images/research/SFU22SepidehPaper.jpg" title="Paper" /></a></td>
		<td><a href="https://youtu.be/0dy-kzUwsKo" target="_blank"><img src="http://yaksoy.github.io/images/research/SFU22SepidehVideo.jpg" title="Video" /></a></td>
	</tr></table>
	</div>
  
  <input id="SFU22b-item3" name="accordion1" type="checkbox" />
  <label for="SFU22b-item3">BibTeX</label>
  <div class="pub-bibtex">
	@MASTERSTHESIS{cfp-msc,<br />
	author={Sepideh Sarajian Maralan},<br />
	title={Computational Flash Photography},<br />
	year={2022},<br />
	school={Simon Fraser University},<br />
  }
  </div>
</div>
</td>
</tr>
</table>

<hr />

<table>
<tr>
<td class="pub-photocol">
<a href="http://yaksoy.github.io/bmd-msc/" target="_blank"><img src="http://yaksoy.github.io/images/research/mahdimsc.jpg" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="http://yaksoy.github.io/bmd-msc/" target="_blank">Boosting Monocular Depth Estimation to High Resolution <i class="fa fa-external-link"></i></a>
</div>
<div class="pub-authors">
	<b>Seyed Mahdi Hosseini Miangoleh</b>
</div>
<div class="pub-venue">
	MSc Thesis, Simon Fraser University, 2022
</div>

<div class="accordion">
	<input id="SFU22a-item1" name="accordion1" type="checkbox" />
	<label for="SFU22a-item1">Abstract</label>
	<div class="pub-abstract">
		Convolutional neural networks have shown a remarkable ability to estimate depth from a single image. 
		However, the estimated depth maps are low resolution due to network structure and hardware limitations, only showing the overall scene structure and lacking fine details, which limits their applicability. 
		We demonstrate that there is a trade-off between the consistency of the scene structure and the high-frequency details concerning input content and resolution. 
		Building upon this duality, we present a double estimation framework to improve the depth estimation of the whole image and a patch selection step to add more local details. 
		Our approach obtains multi-megapixel depth estimations with sharp details by merging estimations at different resolutions based on image content. 
		A key strength of our approach is that we can employ any off-the-shelf pre-trained CNN-based monocular depth estimation model without requiring further finetuning.
  </div>
  
  <input id="SFU22a-item2" name="accordion1" type="checkbox" />
  <label for="SFU22a-item2">Thesis and Presentation</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="https://sfu.ca/~smh31/masterthesis" target="_blank"><img src="http://yaksoy.github.io/images/research/SFU22MahdiPaper.jpg" title="Paper" /></a></td>
		<td><a href="https://youtu.be/DZ0ft1l50KY" target="_blank"><img src="http://yaksoy.github.io/images/research/SFU22MahdiVideo.jpg" title="Video" /></a></td>
	</tr></table>
	</div>
  
  <input id="SFU22a-item3" name="accordion1" type="checkbox" />
  <label for="SFU22a-item3">BibTeX</label>
  <div class="pub-bibtex">
	@MASTERSTHESIS{bmd-msc,<br />
	author={Seyed Mahdi Hosseini Miangoleh},<br />
	title={Boosting Monocular Depth Estimation to High Resolution},<br />
	year={2022},<br />
	school={Simon Fraser University},<br />
  }
  </div>
</div>
</td>
</tr>
</table>

<hr />

<table>
<tr>
<td class="pub-photocol">
<a href="http://yaksoy.github.io/ssi/" target="_blank"><img src="http://yaksoy.github.io/images/jorge.png" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="http://yaksoy.github.io/ssi/" target="_blank">Soft Segmentation of Images <i class="fa fa-external-link"></i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a>
</div>
<div class="pub-authors">
	<b>Yağız Aksoy</b>
</div>
<div class="pub-venue">
	PhD Thesis, ETH Zurich, 2019
</div>

<div class="accordion">
  <input id="ETH19-item2" name="accordion1" type="checkbox" />
  <label for="ETH19-item2">Dissertation</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="http://yaksoy.github.io/papers/ETH19-PhD-Aksoy.pdf" target="_blank"><img src="http://yaksoy.github.io/images/research/ETH19Paper.png" title="Paper" /></a></td>
	</tr></table>
	</div>
  
  <input id="ETH19-item3" name="accordion1" type="checkbox" />
  <label for="ETH19-item3">BibTeX</label>
  <div class="pub-bibtex">
		@phdthesis{ssi,<br />
			author={Ya\u{g}{\i}z Aksoy},<br />
			title={Soft Segmentation of Images},<br />
			year={2019},<br />
			school={ETH Zurich},<br />
			}	 
  </div>
</div>
</td>
</tr>
</table>

<hr />

<table>
<tr>
<td class="pub-photocol">
<a href="http://yaksoy.github.io/papers/METU13-msc.pdf" target="_blank"><img src="http://yaksoy.github.io/images/research/METU13.png" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="http://yaksoy.github.io/papers/METU13-msc.pdf" target="_blank">Efficient Inertially Aided Visual Odometry towards Mobile Augmented Reality</a>
</div>
<div class="pub-authors">
	<b>Yağız Aksoy</b>
</div>
<div class="pub-venue">
	Master's Thesis, Middle East Technical University, 2013
</div>

<div class="accordion">
  <input id="METU13-item1" name="accordion1" type="checkbox" />
  <label for="METU13-item1">Abstract</label>
  <div class="pub-abstract">
  With the increase in the number and computational power of commercial mobile devices like smart phones and tablet computers, augmented reality applications are gaining more and more volume. In order to augment virtual objects effectively in real scenes, pose of the camera should be estimated with high precision and speed. Today, most of the mobile devices feature cameras and inertial measurement units which carry information on change in position and attitude of the camera. In this thesis, utilization of inertial sensors on mobile devices in aiding visual pose estimation is studied. Error characteristics of the inertial sensors on the utilized mobile device are analyzed. Gyroscope readings are utilized for aiding 2D feature tracking while accelerometer readings are used to help create a sparse 3D map of features later to be used for visual pose estimation. Metric velocity estimation is formulated using inertial readings and observations of a single 2D feature. Detailed formulations of uncertainties on all the estimated variables are provided. Finally, a novel, lightweight filter, which is capable of estimating the pose of the camera together with the metric scale, is proposed. The proposed filter runs without any heuristics needed for covariance propagation, which enables it to be used in different devices with different sensor characteristics without any modifications. Sensors with poor noise characteristics are successfully utilized to aid the visual pose estimation.
  </div>

  <input id="METU13-item2" name="accordion1" type="checkbox" />
  <label for="METU13-item2">Manuscript</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="http://yaksoy.github.io/papers/METU13-msc.pdf" target="_blank"><img src="http://yaksoy.github.io/images/research/METU13Paper.png" title="Paper" /></a></td>
	</tr></table>
	</div>
  
  <input id="METU13-item3" name="accordion1" type="checkbox" />
  <label for="METU13-item3">BibTeX</label>
  <div class="pub-bibtex">
	@MASTERSTHESIS{yaksoymetu13,<br />
	author = {Aksoy, Ya\u{g}{\i}z},<br />
	title = {Efficient Inertially Aided Visual Odometry towards Mobile Augmented Reality},<br />
	school = {Middle East Technical University},<br />
	year = {2013},<br />
	month = {August}}
  </div>
</div>
</td>
</tr>
</table>

<hr />

<h4>Other conferences and workshops</h4>

<hr />

<table>
<tr>
<td class="pub-photocol">
<a href="../interactiveDepth" target="_blank"><img src="http://yaksoy.github.io/images/research/interactiveDepth.jpg" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="../interactiveDepth" target="_blank">Interactive Editing of Monocular Depth <i class="fa fa-external-link"></i></a>
</div>
<div class="pub-authors">
	<b>Obumneme Stanley Dukor</b>, <b>S. Mahdi H. Miangoleh</b>, <b>Mahesh Kumar Krishna Reddy</b>, Long Mai, and <b>Yağız Aksoy</b> 
</div>
<div class="pub-venue">
	SIGGRAPH Posters, 2022
</div>

<div class="accordion">
  <input id="SIG22a-item1" name="accordion1" type="checkbox" />
  <label for="SIG22a-item1">Abstract</label>
  <div class="pub-abstract">
	Recent advances in computer vision have made 3D structure-aware editing of still photographs a reality. 
	Such computational photography applications use a depth map that is automatically generated by monocular depth estimation methods to represent the scene structure. 
	In this work, we present a lightweight, web-based interactive depth editing and visualization tool that adapts low-level conventional image editing operations for geometric manipulation to enable artistic control in the 3D photography workflow. 
	Our tool provides real-time feedback on the geometry through a 3D scene visualization to make the depth map editing process more intuitive for artists. 
	Our web-based tool is open-source and platform-independent to support wider adoption of 3D photography techniques in everyday digital photography.
  </div>

  <input id="SIG22a-item2" name="accordion1" type="checkbox" />
  <label for="SIG22a-item2">Manuscript &amp; more</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="http://yaksoy.github.io/papers/SIG22a-interactiveDepth.pdf" target="_blank"><img width="200" src="http://yaksoy.github.io/images/research/interactiveDepthPaper.jpg" title="Paper" /></a></td>
		<td><a href="http://yaksoy.github.io/papers/SIG22a-interactiveDepth.jpg" target="_blank"><img width="200" src="http://yaksoy.github.io/images/research/interactiveDepthPosterSmall.jpg" title="Poster" /></a></td>
		<td><a href="" target="_blank"><img src="http://yaksoy.github.io/images/research/" title="Video" /></a></td>
	</tr></table>
	</div>

  <input id="SIG22a-item3" name="accordion1" type="checkbox" />
  <label for="SIG22a-item3">BibTeX</label>
  <div class="pub-bibtex">
	@INPROCEEDINGS{interactiveDepth,<br />
	author={Obumneme Stanley Dukor and S. Mahdi H. Miangoleh and Mahesh Kumar Krishna Reddy and Long Mai and Ya\u{g}{\i}z Aksoy},<br />
	title={Interactive Editing of Monocular Depth},<br />
	booktitle={SIGGRAPH Posters},<br />
	year={2022},<br />
	}
  </div>
</div>
</td>
</tr>
</table>

<hr />

<table>
<tr>
<td class="pub-photocol">
<a href="../dynapix" target="_blank"><img src="http://yaksoy.github.io/images/research/dynapix.jpg" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="../dynapix" target="_blank">DynaPix: Normal Map Pixelization for Dynamic Lighting <i class="fa fa-external-link"></i></a>
</div>
<div class="pub-authors">
	Gerardo Gandeaga, Denys Iliash, <b>Chris Careaga</b>, and <b>Yağız Aksoy</b> 
</div>
<div class="pub-venue">
	SIGGRAPH Posters, 2022
</div>

<div class="accordion">
  <input id="SIG22b-item1" name="accordion1" type="checkbox" />
  <label for="SIG22b-item1">Abstract</label>
  <div class="pub-abstract">
	This work introduces DynaPix, a Krita extension that automatically generates pixelated images and surface normals from an input image. 
	DynaPix is a tool that aids pixel artists and game developers more efficiently develop 8-bit style games and bring them to life with dynamic lighting through normal maps that can be used in modern game engines such as Unity. 
	The extension offers artists a degree of flexibility as well as allows for further refinements to generated artwork. 
	Powered by out of the box solutions, DynaPix is a tool that seamlessly integrates in the artistic workflow.
  </div>

  <input id="SIG22b-item2" name="accordion1" type="checkbox" />
  <label for="SIG22b-item2">Manuscript &amp; more</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="http://yaksoy.github.io/papers/SIG22b-DynaPix.pdf" target="_blank"><img width="200" src="http://yaksoy.github.io/images/research/dynapixPaper.jpg" title="Paper" /></a></td>
		<td><a href="http://yaksoy.github.io/papers/SIG22b-DynaPix.jpg" target="_blank"><img width="200" src="http://yaksoy.github.io/images/research/dynapixPosterSmall.jpg" title="Poster" /></a></td>
		<td><a href="https://youtu.be/1mylyzw6i_U" target="_blank"><img src="http://yaksoy.github.io/images/research/dynapixVideo.jpg" title="Video" /></a></td>
	</tr></table>
	</div>

  <input id="SIG22b-item3" name="accordion1" type="checkbox" />
  <label for="SIG22b-item3">BibTeX</label>
  <div class="pub-bibtex">
	@INPROCEEDINGS{dynapix,<br />
	author={Gerardo Gandeaga and Denys Iliash and Chris Careaga and Ya\u{g}{\i}z Aksoy},<br />
	title={Dyna{P}ix: Normal Map Pixelization for Dynamic Lighting},<br />
	booktitle={SIGGRAPH Posters},<br />
	year={2022},<br />
	}
  </div>
</div>
</td>
</tr>
</table>

<hr />

<table>
<tr>
<td class="pub-photocol">
<a href="../parallaxbg" target="_blank"><img src="http://yaksoy.github.io/images/research/parallax.jpg" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="../parallaxbg" target="_blank">Parallax Background Texture Generation <i class="fa fa-external-link"></i></a>
</div>
<div class="pub-authors">
	Brigham Okano, Shao Yu Shen, <b>Sebastian Dille</b>, and <b>Yağız Aksoy</b> 
</div>
<div class="pub-venue">
	SIGGRAPH Posters, 2022
</div>

<div class="accordion">
  <input id="SIG22c-item1" name="accordion1" type="checkbox" />
  <label for="SIG22c-item1">Abstract</label>
  <div class="pub-abstract">
	Art assets for games can be time intensive to produce. 
	Whether it is a full 3D world, or simpler 2D background, creating good looking assets takes time and skills that are not always readily available. 
	Time can be saved by using repeating assets, but visible repetition hurts immersion. 
	Procedural generation techniques can help make repetition less uniform, but do not remove it entirely. 
	Both approaches leave noticeable levels of repetition in the image, and require significant time and skill investments to produce. 
	Video game developers in hobby, game jam, or early prototyping situations may not have access to the required time and skill. 
	We propose a framework to produce layered 2D backgrounds without the need for significant artist time or skill. 
	In our pipeline, the user provides segmented photographic input, instead of creating traditional art, and receives game-ready assets. 
	By utilizing photographs as input, we can achieve both a high level of realism for the resulting background texture as well as a shift from manual work away towards computational run-time which frees up developers for other work.
  </div>

  <input id="SIG22c-item2" name="accordion1" type="checkbox" />
  <label for="SIG22c-item2">Manuscript &amp; more</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="http://yaksoy.github.io/papers/SIG22c-ParallaxBG.pdf" target="_blank"><img width="200" src="http://yaksoy.github.io/images/research/parallaxPaper.jpg" title="Paper" /></a></td>
		<td><a href="http://yaksoy.github.io/papers/SIG22c-ParallaxBG.jpg" target="_blank"><img width="200" src="http://yaksoy.github.io/images/research/parallaxPosterSmall.jpg" title="Poster" /></a></td>
		<td><a href="https://youtu.be/_KWdFy3YipI" target="_blank"><img src="http://yaksoy.github.io/images/research/parallaxVideo.jpg" title="Video" /></a></td>
	</tr></table>
	</div>

  <input id="SIG22c-item3" name="accordion1" type="checkbox" />
  <label for="SIG22c-item3">BibTeX</label>
  <div class="pub-bibtex">
	@INPROCEEDINGS{parallaxBG,<br />
	author={Brigham Okano and Shao Yu Shen and Sebastian Dille and Ya\u{g}{\i}z Aksoy},<br />
	title={Parallax Background Texture Generation},<br />
	booktitle={SIGGRAPH Posters},<br />
	year={2022},<br />
	}
  </div>
</div>
</td>
</tr>
</table>

<hr />

<table>
<tr>
<td class="pub-photocol">
  <a href="https://www.disneyresearch.com/publication/ar-museum/" target="_blank"><img src="http://yaksoy.github.io/images/research/ARmuseum.png" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="https://www.disneyresearch.com/publication/ar-museum/" target="_blank">AR Museum: A Mobile Augmented Reality Application for Interactive Painting Recoloring <i class="fa fa-external-link"></i></a>
</div>
<div class="pub-authors">
  Mattia Ryffel, Fabio Zünd, <b>Yağız Aksoy</b>, Alessia Marra, Maurizio Nitti, Tunç Ozan Aydın and Bob Sumner
</div>
<div class="pub-venue">
	International Conference on Game and Entertainment Technologies, 2017
</div>

<div class="accordion">
  <input id="GET17-item1" name="accordion1" type="checkbox" />
  <label for="GET17-item1">Abstract</label>
  <div class="pub-abstract">
    We present a mobile augmented reality application that allows its users to modify colors of paintings via simple touch
    interactions. Our method is intended for museums and art exhibitions and aims to provide an entertaining way for
    interacting with paintings in a non-intrusive manner. Plausible color edits are achieved by utilizing a set of layers with
    corresponding alpha channels, which needs to be generated for each individual painting in a pre-processing step.
    Manually performing such a layer decomposition is a tedious process and makes the entire system infeasible for most
    practical use cases. In this work, we propose the use of a fully automatic soft color segmentation algorithm for content
    generation for such an augmented reality application. This way, we significantly reduce the amount of manual labor
    needed for deploying our system and thus make our system feasible for real-world use.
  </div>

  <input id="GET17-item2" name="accordion1" type="checkbox" />
  <label for="GET17-item2">Manuscript </label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="http://zurich.disneyresearch.com/~taydin/_resources/publication/armuseum.pdf" target="_blank"><img width="200" src="http://yaksoy.github.io/images/research/GET17paper.png" title="Paper" /></a></td>
	</tr></table>
	</div>

  <input id="GET17-item3" name="accordion1" type="checkbox" />
  <label for="GET17-item3">BibTeX</label>
  <div class="pub-bibtex">
	@INPROCEEDINGS{armuseum,<br />
	author={Mattia Ryffel and Fabio Z\"und and Ya\u{g}{\i}z Aksoy and Alessia Marra and Maurizio Nitti and Tun\c{c} Ozan Ayd{\i}n and Bob Sumner},<br />
	title={AR Museum: A Mobile Augmented Reality Application for Interactive Painting Recoloring},<br />
	booktitle={International Conference on Game and Entertainment Technologies},<br />
	year={2017},<br />
  }
  </div>
</div>
</td>
</tr>
</table>

<hr />

<table>
<tr>
<td class="pub-photocol">
<a href="http://yaksoy.github.io/papers/ICIP14-mastercam.pdf" target="_blank"><img src="http://yaksoy.github.io/images/research/ICIP14b.jpg" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="http://yaksoy.github.io/papers/ICIP14-mastercam.pdf" target="_blank">Mastercam FVV: Robust Registration of Multiview Sports Video to a Static High-Resolution Master Camera for Free Viewpoint Video</a>
</div>
<div class="pub-authors">
	Florian Angehrn, Oliver Wang, <b>Yağız Aksoy</b>, Markus Gross and Aljosa Smolic
</div>
<div class="pub-venue">
	IEEE ICIP, 2014
</div>

<div class="accordion">
  <input id="ICIP14b-item1" name="accordion1" type="checkbox" />
  <label for="ICIP14b-item1">Abstract</label>
  <div class="pub-abstract">
  Free viewpoint video enables interactive viewpoint selection in real world scenes, which is attractive for many applications such as sports visualization. Multi-camera registration is one of the difficult tasks in such systems. We introduce the concept of a static high resolution master camera for improved long-term multiview alignment. All broadcast cameras are aligned to a common reference. Our approach builds on frame-to-frame alignment, extended into a recursive long-term estimation process, which is shown to be accurate, robust and stable over long sequences.
  </div>

  <input id="ICIP14b-item2" name="accordion1" type="checkbox" />
  <label for="ICIP14b-item2">Manuscript</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="http://yaksoy.github.io/papers/ICIP14-mastercam.pdf" target="_blank"><img src="http://yaksoy.github.io/images/research/ICIP14bPaper.png" title="Paper" /></a></td>
	</tr></table>
	</div>
  
  <input id="ICIP14b-item3" name="accordion1" type="checkbox" />
  <label for="ICIP14b-item3">BibTeX</label>
  <div class="pub-bibtex">
	@INPROCEEDINGS{anghernicip14,<br />
	author={Florian Angehrn and Oliver Wang and Ya\u{g}{\i}z Aksoy and Markus Gross and Aljo\v{s}a Smoli\'{c}},<br />
	title={MasterCam FVV: Robust Registration of Multiview Sports Video to a Static High-Resolution Master Camera for Free Viewpoint Video},<br />
	booktitle={IEEE International Conference on Image Processing (ICIP)},<br />
	year={2014}}
  </div>
</div>
</td>
</tr>
</table>

<hr />

<table>
<tr>
<td class="pub-photocol">
<a href="http://yaksoy.github.io/papers/ICIP14-odometry.pdf" target="_blank"><img src="http://yaksoy.github.io/images/research/ICIP14a.jpg" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="http://yaksoy.github.io/papers/ICIP14-odometry.pdf" target="_blank">Uncertainty Modeling for Efficient Visual Odometry via Inertial Sensors on Mobile Devices</a>
</div>
<div class="pub-authors">
	<b>Yağız Aksoy</b> and A. Aydın Alatan
</div>
<div class="pub-venue">
	IEEE ICIP, 2014
</div>

<div class="accordion">
  <input id="ICIP14a-item1" name="accordion1" type="checkbox" />
  <label for="ICIP14a-item1">Abstract</label>
  <div class="pub-abstract">
  Most of the mobile applications require efficient and precise computation of the device pose, and almost every mobile device has inertial sensors already equipped together with a camera. This fact makes sensor fusion quite attractive for increasing efficiency during pose tracking. However, the state-of-the-art fusion algorithms have a major shortcoming: lack of well-defined uncertainty introduced to the system during the prediction stage of the fusion filters. Such a drawback results in determining covariances heuristically, and hence, requirement for data-dependent tuning to achieve high performance or even convergence of these filters. In this paper, we propose an inertially-aided visual odometry system that requires neither heuristics nor parameter tuning; computation of the required uncertainties on all the estimated variables are obtained after minimum number of assumptions. Moreover, the proposed system simultaneously estimates the metric scale of the pose computed from a monocular image stream. The experimental results indicate that the proposed scale estimation outperforms the state-of-the-art methods, whereas the pose estimation step yields quite acceptable results in real-time on resource constrained systems.
  </div>

  <input id="ICIP14a-item2" name="accordion1" type="checkbox" />
  <label for="ICIP14a-item2">Manuscript &amp; more</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="http://yaksoy.github.io/papers/ICIP14-odometry.pdf" target="_blank"><img src="http://yaksoy.github.io/images/research/ICIP14aPaper.png" title="Paper" /></a></td>
		<td><a href="http://yaksoy.github.io/papers/ICIP14-supp-velocity.pdf" target="_blank"><img src="http://yaksoy.github.io/images/research/ICIP14aVelocity.png" title="Supplementary material 1" /></a></td>
		<td><a href="http://yaksoy.github.io/papers/ICIP14-supp-noise.pdf" target="_blank"><img src="http://yaksoy.github.io/images/research/ICIP14aNoise.png" title="Supplementary material 2" /></a></td>
		<td><a href="http://yaksoy.github.io/papers/ICIP14-PosterSmall.jpg" target="_blank"><img src="http://yaksoy.github.io/images/research/ICIP14aPoster.jpg" title="Poster" /></a></td>
	</tr></table>
	</div>
  
  <input id="ICIP14a-item3" name="accordion1" type="checkbox" />
  <label for="ICIP14a-item3">BibTeX</label>
  <div class="pub-bibtex">
	@INPROCEEDINGS{yaksoyicip14a,<br />
	author={Aksoy, Ya\u{g}{\i}z and Alatan, A. Ayd{\i}n},<br />
	title={Uncertainty Modeling for Efficient Visual Odometry via Inertial Sensors on Mobile Devices},<br />
	booktitle={IEEE International Conference on Image Processing (ICIP)},<br />
	year={2014}}
  </div>
</div>
</td>
</tr>
</table>

<hr />

<table>
<tr>
<td class="pub-photocol">
<img src="http://yaksoy.github.io/images/research/doppler.jpg" class="pub-photo" />
</td>
<td>
<div class="pub-title">
	Impact of transrectal prostate needle biopsy on erectile function: Results of power Doppler ultrasonography of the prostate
</div>
<div class="pub-authors">
	Altug Tuncel, Ugur Toprak, Melih Balci, Ersin Koseoglu, <b>Yağız Aksoy</b>, Alp Karademir and Ali Atan
</div>
<div class="pub-venue">
	The Kaohsiung Journal of Medical Sciences, 2014
</div>

<div class="accordion">
  <input id="KJMC14-item1" name="accordion1" type="checkbox" />
  <label for="KJMC14-item1">Abstract</label>
  <div class="pub-abstract">
  We evaluated the impact of transrectal prostate needle biopsy (TPNB) on erectile function and on the prostate and bilateral neurovascular bundles using power Doppler ultrasonography imaging of the prostate. The study consisted of 42 patients who had undergone TPNB. Erectile function was evaluated prior to the biopsy, and in the 3rd month after the biopsy using the first five-item version of the International Index of Erectile Function (IIEF-5). Prior to and 3 months after the biopsy, the resistivity index of the prostate parenchyma and both neurovascular bundles was measured. The mean age of the men was 64.2 (47–78) years. Prior to TPNB, 10 (23.8%) patients did not have erectile dysfunction (ED) and 32 (76.2%) patients had ED. The mean IIEF-5 score was 20.8 (range: 2–25) prior to the biopsies, and the mean IIEF-5 score was 17.4 (range: 5–25; p &lt; 0.001) after 3 months. For patients who were previously potent in the pre-biopsy period, the ED rate was 40% (n = 4/10) at the 3rd month evaluation. In these patients, all the resistivity index values were significantly decreased. Our results showed that TPNB may lead to an increased risk of ED. The presence of ED in men after TPNB might have an organic basis.
  </div>
  <input id="KJMC14-item3" name="accordion1" type="checkbox" />
  <label for="KJMC14-item3">BibTeX</label>
  <div class="pub-bibtex">
@ARTICLE{Tuncel2014,<br />
title = "Impact of transrectal prostate needle biopsy on erectile function: Results of power Doppler ultrasonography of the prostate ",<br />
author = "Altug Tuncel and Ugur Toprak and Melih Balci and Ersin Koseoglu and Yagiz Aksoy and Alp Karademir and Ali Atan",<br />
journal = "The Kaohsiung Journal of Medical Sciences ",<br />
volume = "30",<br />
number = "4",<br />
pages = "194 - 199",<br />
year = "2014",}
  </div>
</div>
</td>
</tr>
</table>

<hr />

<table>
<tr>
<td class="pub-photocol">
<a href="http://yaksoy.github.io/papers/ECCVW12-shadow.pdf" target="_blank"><img src="http://yaksoy.github.io/images/research/ECCVW12.jpg" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="http://yaksoy.github.io/papers/ECCVW12-shadow.pdf" target="_blank">Utilization of False Color Images in Shadow Detection</a>
</div>
<div class="pub-authors">
	<b>Yağız Aksoy</b> and A. Aydın Alatan
</div>
<div class="pub-venue">
	ECCV Workshops, 2012
</div>

<div class="accordion">
  <input id="ECCVW12-item1" name="accordion1" type="checkbox" />
  <label for="ECCVW12-item1">Abstract</label>
  <div class="pub-abstract">
  Shadows are illuminated as a result of Rayleigh scattering phenomenon, which happens to be more effective for small wavelengths of light. We propose utilization of false color images for shadow detection, since the transformation eliminates high frequency blue component and introduces low frequency near-infrared channel. Effectiveness of the approach is tested by using several shadow-variant texture and color-related cues proposed in the literature. Performances of these cues in regular and false color images are compared and analyzed within a supervised system by using a support vector machine classifier. 
  </div>

  <input id="ECCVW12-item2" name="accordion1" type="checkbox" />
  <label for="ECCVW12-item2">Manuscript</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="http://yaksoy.github.io/papers/ECCVW12-shadow.pdf" target="_blank"><img src="http://yaksoy.github.io/images/research/ECCVW12Paper.png" title="Paper" /></a></td>
	</tr></table>
	</div>
  
  <input id="ECCVW12-item3" name="accordion1" type="checkbox" />
  <label for="ECCVW12-item3">BibTeX</label>
  <div class="pub-bibtex">
	@INPROCEEDINGS{yaksoyeccvw12,<br />
	author={Aksoy, Yagiz and Alatan, A. Aydin},<br />
	title={Utilization of False Color Images in Shadow Detection},<br />
	booktitle={European Conference on Computer Vision (ECCV) Workshops},<br />
	year={2012}}
  </div>
</div>
</td>
</tr>
</table>

<hr />

<table>
<tr>
<td class="pub-photocol">
<a href="http://yaksoy.github.io/papers/ICIP12-2d3dConv.pdf" target="_blank"><img src="http://yaksoy.github.io/images/research/ICIP12.jpg" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="http://yaksoy.github.io/papers/ICIP12-2d3dConv.pdf" target="_blank">Interactive 2D-3D Image Conversion for Mobile Devices</a>
</div>
<div class="pub-authors">
	<b>Yağız Aksoy</b>, Ozan Şener, A. Aydın Alatan and Kemal Uğur
</div>
<div class="pub-venue">
	IEEE ICIP, 2012
</div>

<div class="accordion">
  <input id="ICIP12-item1" name="accordion1" type="checkbox" />
  <label for="ICIP12-item1">Abstract</label>
  <div class="pub-abstract">
  We propose a complete still image based 2D-3D mobile conversion system for touch screen use. The system consists of interactive segmentation followed by 3D rendering. The interactive segmentation is conducted dynamically by color Gaussian mixture model updates and dynamic-iterative graph-cut. A coloring gesture is used to guide the way and entertain the user during the process. Output of the image segmentation is then fed to the 3D rendering stage of the system. For rendering stage, two novel improvements are proposed to handle holes resulting from depth image based rendering process. These improvements are also expected to enhance the 3D perception. These two methods are subjectively tested and their results are presented.
  </div>

  <input id="ICIP12-item2" name="accordion1" type="checkbox" />
  <label for="ICIP12-item2">Manuscript &amp; more</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="http://yaksoy.github.io/papers/ICIP12-2d3dConv.pdf" target="_blank"><img src="http://yaksoy.github.io/images/research/ICIP12Paper.png" title="Paper" /></a></td>
		<td><a href="http://yaksoy.github.io/papers/ICIP12-PosterSmall.png" target="_blank"><img src="http://yaksoy.github.io/images/research/ICIP12Poster.png" title="Poster" /></a></td>
	</tr></table>
	</div>
  
  <input id="ICIP12-item3" name="accordion1" type="checkbox" />
  <label for="ICIP12-item3">BibTeX</label>
  <div class="pub-bibtex">
	@INPROCEEDINGS{yaksoyicip12,<br />
	author={Aksoy, Yagiz and Sener, Ozan and Alatan, A. Aydin and Ugur, Kemal},<br />
	title={Interactive 2D-3D Image Conversion for Mobile Devices},<br />
	booktitle={IEEE International Conference on Image Processing (ICIP)},<br />
	year={2012}}
  </div>
</div>
</td>
</tr>
</table>

<hr />

      </div><!-- /.entry-content -->
    </div><!-- /.entry-wrapper -->
  </article>
</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo" class="entry-wrapper">
    

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
<span>&copy; 2023 Yagiz Aksoy <!--Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a href="http://mademistakes.com/so-simple/" rel="nofollow">So Simple Theme</a>.--></span>
<div class="social-icons">
	<a href="https://scholar.google.com/citations?user=AC445kUAAAAJ" title="Yagiz Aksoy on Google Scholar" target="_blank"><img src="http://yaksoy.github.io/images/logos/scholar.png" alt="Scholar" width="20"></img></a>
	<a href="http://dblp.uni-trier.de/pers/hd/a/Aksoy:Yagiz" title="Yagiz Aksoy on DBLP" target="_blank"><img src="http://yaksoy.github.io/images/logos/dblp.png" alt="DBLP" width="20"></img></a>
	<a href="https://www.researchgate.net/profile/Yagiz_Aksoy" title="Yagiz Aksoy on ResearchGate" target="_blank"><img src="http://yaksoy.github.io/images/logos/rgnew.png" alt="RG" width="20"></img></a>
	<a href="http://orcid.org/0000-0002-1495-0491" title="Yagiz Aksoy on ORCID" target="_blank"><img src="http://yaksoy.github.io/images/logos/orcid.png" alt="ORCID" width="20"></img></a>
	<br><br>
	<a href="http://twitter.com/yagizaksoy" title="Yagiz Aksoy on Twitter" target="_blank"><i class="fa fa-twitter-square fa-2x"></i></a>
	<a href="https://www.youtube.com/channel/UCUmv_OnSSWMCkOESVd6607w" title="Yagiz Aksoy on YouTube" target="_blank"><i class="fa fa-youtube-play fa-2x"></i></a>
	
	
	<a href="http://linkedin.com/in/yaksoy" title="Yagiz Aksoy on LinkedIn" target="_blank"><i class="fa fa-linkedin-square fa-2x"></i></a>
	
	
	
	
	<a href="http://github.com/yaksoy" title="Yagiz Aksoy on Github" target="_blank"><i class="fa fa-github-square fa-2x"></i></a>
	
  
	
  <!--<a href="http://yaksoy.github.io/feed.xml" title="Atom/RSS feed"><i class="fa fa-rss-square fa-2x"></i></a>-->
</div><!-- /.social-icons -->

  </footer>
</div><!-- /.footer-wrapper -->

<script type="text/javascript">
  var BASE_URL = 'http://yaksoy.github.io';
</script>

<!-- Include Latex style math -->
<!-- https://stackoverflow.com/questions/10987992/using-mathjax-with-jekyll -->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://yaksoy.github.io/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://yaksoy.github.io/assets/js/scripts.min.js"></script>



<!-- Start of StatCounter Code for Default Guide -->
<script type="text/javascript">
var sc_project=11195487; 
var sc_invisible=1; 
var sc_security="112d9b46"; 
var scJsHost = (("https:" == document.location.protocol) ?
"https://secure." : "http://www.");
document.write("<sc"+"ript type='text/javascript' src='" +
scJsHost+
"statcounter.com/counter/counter.js'></"+"script>");
</script>
<noscript><div class="statcounter"><a title="shopify
analytics" href="http://statcounter.com/shopify/"
target="_blank"><img class="statcounter"
src="//c.statcounter.com/11195487/0/112d9b46/1/"
alt="shopify analytics"></a></div></noscript>
<!-- End of StatCounter Code for Default Guide -->

</body>
</html>
