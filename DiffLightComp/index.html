<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Interactive Object Insertion with Differentiable Rendering &#8211; Yağız Aksoy</title>
<meta name="description" content="Interactive Object Insertion with Differentiable Rendering">



<!-- Twitter Cards -->
<meta name="twitter:title" content="Interactive Object Insertion with Differentiable Rendering">
<meta name="twitter:description" content="Interactive Object Insertion with Differentiable Rendering">



<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://yaksoy.github.io/images/Sig25Poster-LightCompositingTeaser.jpg">

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Interactive Object Insertion with Differentiable Rendering">
<meta property="og:description" content="Interactive Object Insertion with Differentiable Rendering">
<meta property="og:url" content="https://yaksoy.github.io/DiffLightComp/">
<meta property="og:site_name" content="Yağız Aksoy">

<!-- Webmaster Tools verfication -->
<meta name="google-site-verification" content="googleb0479c04a25255c3">



<link rel="canonical" href="https://yaksoy.github.io/DiffLightComp/">
<link href="https://yaksoy.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Yağız Aksoy Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="https://yaksoy.github.io/assets/css/main.css">
<!-- Webfonts -->
<script src="//use.edgefonts.net/source-sans-pro:n2,i2,n3,i3,n4,i4,n6,i6,n7,i7,n9,i9;source-code-pro:n4,n7;volkhov.js"></script>

<meta http-equiv="cleartype" content="on">

<!-- HTML5 Shiv and Media Query Support -->
<!--[if lt IE 9]>
  <script src="https://yaksoy.github.io/assets/js/vendor/html5shiv.min.js"></script>
  <script src="https://yaksoy.github.io/assets/js/vendor/respond.min.js"></script>
<![endif]-->

<!-- Modernizr -->
<script src="https://yaksoy.github.io/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="https://yaksoy.github.io/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="https://yaksoy.github.io/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="https://yaksoy.github.io/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="https://yaksoy.github.io/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="https://yaksoy.github.io/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://yaksoy.github.io/images/apple-touch-icon-144x144-precomposed.png">

</head>

<body id="page">

<div id="main" role="main">
  <article class="entry">
    <div class="entry-wrapper">
      <br>
      <div class="entry-titleLogos">
        <table><colgroup><col style="width:50%" /><col style="width:50%" /></colgroup><tr><td style="text-align:left"><a href="https://www.sfu.ca/computing.html" target="_blank"><img src="../images/sfu.png" class="proj-logo" title="SFU"></a></td><td style="text-align:right"><a href="../group/" target="_blank"><img src="../images/cplab-full.png" class="proj-logo" title="CPLab" style="float:right;"></a></td></tr></table>
      </div>
      <br>
      <!-- <header class="entry-header">-->
        <h1 class="entry-titleProject" style="margin-top:5px;">Interactive Object Insertion with Differentiable Rendering</h1>
      <!-- </header>-->
      <div class="entry-titleAuthors">
        <table style="text-align:center;"><colgroup><col style="width:25%" /><col style="width:25%" /><col style="width:25%" /><col style="width:25%" /></colgroup><tr><td><img src="weikun.jpg" class="proj-photo" title="Weikun"></td><td><img src="sota.jpg" class="proj-photo" title="Sota"></td><td><a href="https://ccareaga.github.io" target="_blank"><img src="../group/chris.jpg" class="proj-photo" title="Chris"></td><td><a href="../" target="_blank"><img src="../images/yagizsq2.jpg" class="proj-photo" title="Yagiz"></td></tr><tr><td>Weikun Peng*</td><td>Sota Taira*</td><td><a href="https://ccareaga.github.io" target="_blank">Chris Careaga</a></td><td><a href="http://yaksoy.github.io" target="_blank">Yağız Aksoy</a></td></tr></table>
      </div>
      <div class="entry-titleVenue">
        SIGGRAPH Posters, 2025
      </div>
    </div><!-- /.entry-wrapper -->
    <img src="https://yaksoy.github.io/images/Sig25Poster-LightCompositingTeaser.jpg" class="entry-feature-image" alt="Interactive Object Insertion with Differentiable Rendering" style="margin-top:-40;"><p class="image-caption">We present an object insertion pipeline and interface that enables iterative editing of illumination-aware composite images. Our pipeline leverages off-the-shelf computer vision methods and differentiable rendering to reconstruct a 3D representation of a given scene. Users can add 3D objects and render them with physically accurate lighting effects.</p>
    <div class="entry-wrapper">
      <div class="entry-contentProject">
        <p style="text-align:center;"><b>Abstract</b></p>

<p>
	Compositing virtual objects into real-world imagery, referred to as object insertion, has a number of applications across film visual effects, augmented reality, and even interior design. Creating physically realistic composites can be a tedious process requiring an artist to perform manual editing of illumination effects for a given object and background scene. This manual process lacks interactive feedback, making it difficult to finetune aspects of the composite, such as object location, size, and orientation. In this work, we propose a modern framework and accompanying user interface to bring recent advancements in computational photography to artists and designers in an accessible and extensible manner. Specifically, we follow the paper <a href="https://yaksoy.github.io/DiffLightComp" target="_blank">Physically Controllable Relighting of Photographs<i class="fa fa-external-link"></i></a> and leverage state-of-the-art mid-level vision estimations to build a virtual 3D scene from a single image. We then use differentiable rendering and optional user constraints to determine the lighting conditions in the scene. Finally, we allow the user to place 3D objects into the scene and render them using the estimated illumination, resulting in a final realistic composite. Our method brings together ideas from the past decade of inverse rendering research to create an open-source tool for artists and designers.
</p>
<p>
	This work was developed by Weikun and Sota as a class project for <a href="https://yaksoy.github.io/cpim" target="_blank">CMPT 461/769 - Computational Photography</a> at SFU.
</p>

<p style="text-align:center;"><b>Implementation</b></p>

<p style="text-align:center;"><a href="https://github.com/willipwk/image-composing" target="_blank">GitHub Repository for CMPT 461/769 - Computational Photography <i class="fa fa-external-link"></i></a></p>

<p style="text-align:center;"><b>Paper</b></p>
<table style="text-align:center;"><tr>
		<td><a href="https://yaksoy.github.io/papers/SIG25p-LightAwareCompositing.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/2025Postera-Paper.jpg" title="Paper" /></a></td>
		<td><a href="https://yaksoy.github.io/papers/SIG25p-LightAwareCompositing.jpg" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/2025Postera-Poster.jpg" title="Poster" /></a></td>
	</tr></table>

<p style="text-align:center;"><b>Video</b></p>

<p style="text-align:center;">
<iframe width="560" height="315" src="https://www.youtube.com/embed/QvYROlkmgGI" frameborder="0"> </iframe>
</p>

<p style="text-align:center;"><b>BibTeX</b></p>
<p>
<div class="pub-bibtex">
	@INPROCEEDINGS{pengTairaCompositing,<br />
	author={Weikun Peng and Sota Taira and Chris Careaga and Ya\u{g}{\i}z Aksoy},<br />
	title={Interactive Object Insertion with Differentiable Rendering},<br />
	booktitle={SIGGRAPH Posters},<br />
	year={2025},<br />
	}
  </div>
  </p>

<p style="text-align:center;"><b>Related Publications</b></p>

<hr />

<table>
	<tr>
	<td class="pub-photocol">
	<a href="../PhysicalRelighting" target="_blank"><img src="https://yaksoy.github.io/images/research/PhysicalRelighting.gif" class="pub-photo" /></a>
	</td>
	<td>
	<div class="pub-title">
		<a href="../PhysicalRelighting" target="_blank">Physically Controllable Relighting of Photographs <i class="fa fa-external-link"></i></a>
	</div>
	<div class="pub-authors">
		<b>Chris Careaga</b> and <b>Yağız Aksoy</b>
	</div>
	<div class="pub-venue">
		Proc. SIGGRAPH, 2025
	</div>
	
	<div class="pub-accordion">
	  <input id="SIG25-item1" name="accordion1" type="checkbox" />
	  <label for="SIG25-item1">Abstract</label>
	  <div class="pub-abstract">
		We present a self-supervised approach to in-the-wild image relighting that enables fully controllable, physically based illumination editing.
		We achieve this by combining the physical accuracy of traditional rendering with the photorealistic appearance made possible by neural rendering.
		Our pipeline works by inferring a colored mesh representation of a given scene using monocular estimates of geometry and intrinsic components.
		This representation allows users to define their desired illumination configuration in 3D. The scene under the new lighting can then be rendered using a path-tracing engine.
		We send this approximate rendering of the scene through a feed-forward neural renderer to predict the final photorealistic relighting result.
		We develop a differentiable rendering process to reconstruct in-the-wild scene illumination, enabling self-supervised training of our neural renderer on raw image collections.
		Our method represents a significant step in bringing the explicit physical control over lights available in typical 3D computer graphics tools, such as Blender, to in-the-wild relighting.
	  </div>
	
	  <input id="SIG25-item2" name="accordion1" type="checkbox" />
	  <label for="SIG25-item2">Manuscript &amp; more</label>
	  <div class="pub-photolink">
	  
		<table><tr>
			<td><a href="https://yaksoy.github.io/papers/SIG25-PhysicalRelighting.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/Sig25Paper.jpg" title="Paper" /></a></td>
			<td><a href="https://yaksoy.github.io/papers/SIG25-PhysicalRelighting-Supp.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/Sig25Supp.jpg" title="Supplementary" /></a></td>
			<td><a href="https://youtu.be/XFJCT3D8t0M" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/Sig25Video.jpg" title="Video" /></a></td>
			<td><a href="https://yaksoy.github.io/PhysicalRelighting/relight_poster.jpg" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/Sig25Poster.jpg" title="Poster" /></a></td>
		</tr></table>
		</div>
	
	  <input id="SIG25-item3" name="accordion1" type="checkbox" />
	  <label for="SIG25-item3">BibTeX</label>
	  <div class="pub-bibtex">
		  @INPROCEEDINGS{careagaRelighting,<br />
		  author={Chris Careaga and Ya\u{g}{\i}z Aksoy},<br />
		  title={Physically Controllable Relighting of Photographs},<br />
		  booktitle={Proc. SIGGRAPH},<br />
		  year={2025},<br />
	  } 
	  </div>
	</div>
	</td>
	</tr>
	</table>

<hr />

<table>
	<tr>
	<td class="pub-photocol">
	<a href="../intrinsicCompositing" target="_blank"><img src="https://yaksoy.github.io/images/research/intrinsicCompositing.jpg" class="pub-photo" /></a>
	</td>
	<td>
	<div class="pub-title">
		<a href="../intrinsicCompositing" target="_blank">Intrinsic Harmonization for Illumination-Aware Compositing <i class="fa fa-external-link"></i></a>
	</div>
	<div class="pub-authors">
		<b>Chris Careaga</b>, <b>S. Mahdi H. Miangoleh</b>, and <b>Yağız Aksoy</b>
	</div>
	<div class="pub-venue">
		SIGGRAPH Asia, 2023
	</div>
	
	<div class="pub-accordion">
	  <input id="SigAsia23-item1" name="accordion1" type="checkbox" />
	  <label for="SigAsia23-item1">Abstract</label>
	  <div class="pub-abstract">
		Despite significant advancements in network-based image harmonization techniques, there still exists a domain disparity between typical training pairs and real-world composites encountered during inference. 
		Most existing methods are trained to reverse global edits made on segmented image regions, which fail to accurately capture the lighting inconsistencies between the foreground and background found in composited images. 
		In this work, we introduce a self-supervised illumination harmonization approach formulated in the intrinsic image domain. 
		First, we estimate a simple global lighting model from mid-level vision representations to generate a rough shading for the foreground region. 
		A network then refines this inferred shading to generate a harmonious re-shading that aligns with the background scene. 
		In order to match the color appearance of the foreground and background, we utilize ideas from prior harmonization approaches to perform parameterized image edits in the albedo domain. 
		To validate the effectiveness of our approach, we present results from challenging real-world composites and conduct a user study to objectively measure the enhanced realism achieved compared to state-of-the-art harmonization methods. 
	  </div>
	
	  <input id="SigAsia23-item2" name="accordion1" type="checkbox" />
	  <label for="SigAsia23-item2">Manuscript &amp; more</label>
	  <div class="pub-photolink">
	  
		<table><tr>
			<td><a href="https://yaksoy.github.io/papers/SigAsia23-IntrinsicCompositing.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/SigAsia23Paper.jpg" title="Paper" /></a></td>
			<td><a href="https://yaksoy.github.io/papers/SigAsia23-IntrinsicCompositing-Supp.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/SigAsia23Supp.jpg" title="Supplementary" /></a></td>
			<td><a href="https://youtu.be/M9hCUTp8bo4" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/SigAsia23Video.jpg" title="Video" /></a></td>
		</tr></table>
		</div>
	
	  <input id="SigAsia23-item3" name="accordion1" type="checkbox" />
	  <label for="SigAsia23-item3">BibTeX</label>
	  <div class="pub-bibtex">
		@INPROCEEDINGS{careagaCompositing,<br />
		author={Chris Careaga and S. Mahdi H. Miangoleh and Ya\u{g}{\i}z Aksoy},<br />
		title={Intrinsic Harmonization for Illumination-Aware Compositing},<br />
		booktitle={Proc. SIGGRAPH Asia},<br />
		year={2023},<br />
	  }
	  </div>
	</div>
	</td>
	</tr>
	</table>

<hr />

<table>
<tr>
<td class="pub-photocol">
	<a href="../2DGraphicsComp" target="_blank"><img src="https://yaksoy.github.io/images/research/2025Poster-LogoCompositing.jpg" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="../2DGraphicsComp" target="_blank">Physically-Based Compositing of 2D Graphics <i class="fa fa-external-link"></i></a>
</div>
<div class="pub-authors">
	Tyrus Tracey, Stefan Diaconu, <b>Sebastian Dille</b>, <b>S. Mahdi H. Miangoleh</b>, and <b>Yağız Aksoy</b> 
</div>
<div class="pub-venue">
	SIGGRAPH Posters, 2025
</div>

<div class="pub-accordion">
  <input id="SIG25b-item1" name="accordion1" type="checkbox" />
  <label for="SIG25b-item1">Abstract</label>
  <div class="pub-abstract">
	We propose an interactive pipeline that enables the seamless integration of a 2D logo into a target image, adapting to the surface geometry and lighting conditions of the scene to ensure realistic appearance.
  </div>

  <input id="SIG25b-item2" name="accordion1" type="checkbox" />
  <label for="SIG25b-item2">Manuscript &amp; more</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="https://yaksoy.github.io/papers/SIG25p-2DGraphicCompositing.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/2025Posterb-Paper.jpg" title="Paper" /></a></td>
		<td><a href="https://yaksoy.github.io/papers/SIG25p-2DGraphicCompositing.jpg" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/2025Posterb-Poster.jpg" title="Poster" /></a></td>
		<td><a href="https://youtu.be/Z8e22OwXgEs" target="_blank"><img src="https://yaksoy.github.io/images/research/2025Posterb-Video.jpg" title="Video" /></a></td>
	</tr></table>
	</div>

  <input id="SIG25b-item3" name="accordion1" type="checkbox" />
  <label for="SIG25b-item3">BibTeX</label>
  <div class="pub-bibtex">
	@INPROCEEDINGS{traceyDiaconuCompositing,<br />
	author={Tyrus Tracey and Stefan Diaconu and Sebastian Dille and S. Mahdi H. Miangoleh and Ya\u{g}{\i}z Aksoy},<br />
	title={Physically-Based Compositing of {2D} Graphics},<br />
	booktitle={SIGGRAPH Posters},<br />
	year={2025},<br />
	}
  </div>
</div>
</td>
</tr>
</table>

<hr />

<p style="text-align:center;"><b>More posters from <a href="https://yaksoy.github.io/cpim" target="_blank">CMPT 461/769: Computational Photography <i class="fa fa-external-link"></i></a></b></p>

<hr />

<table>
<tr>
<td class="pub-photocol">
<a href="../NIREditing" target="_blank"><img src="https://yaksoy.github.io/images/research/NIREditing.jpg" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="../NIREditing" target="_blank">Interactive RGB+NIR Photo Editing <i class="fa fa-external-link"></i></a>
</div>
<div class="pub-authors">
	Samuel Antunes Miranda*, Shahrzad Mirzaei*, Mariam Bebawy*, <b>Sebastian Dille</b>, and <b>Yağız Aksoy</b> 
</div>
<div class="pub-venue">
	SIGGRAPH Posters, 2024
</div>

<div class="pub-accordion">
  <input id="SIGp24-item1" name="accordion1" type="checkbox" />
  <label for="SIGp24-item1">Abstract</label>
  <div class="pub-abstract">
	Near-infrared imagery offers great possibilities for creative image editing. Lying outside the visual spectrum, the NIR information can effectively serve as a fourth color channel to common RGB. 
	Compared to the latter, it shows interesting and complementary behavior: its intensity strongly varies with the surface materials in the scene and is less affected by atmospheric perturbations.
	For these reasons, NIR imaging has been a long-standing topic of interest in research and its integration has been proven successful for applications like false coloring, contrast enhancement, image dehazing, and purification of low-light images. 
	Recent developments in smartphone technology have simplified the capturing process, making NIR data readily available for broader use outside the research community. 
	At the same time, existing tools for NIR processing and manipulation are rare and still limited in functionality. 
	With many solutions lacking specialized features, the editing process is inefficient and cumbersome, making them prone to generate suboptimal results. 
	To tackle this issue, we introduce a simple and intuitive photo editing tool that combines RGB and NIR properties, offering functions tailored specifically for the RGB+NIR combination, and granting the user the ability to edit and refine images more creatively.
  </div>

  <input id="SIGp24-item2" name="accordion1" type="checkbox" />
  <label for="SIGp24-item2">Manuscript &amp; more</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="https://yaksoy.github.io/papers/SIGp24-NIREditing.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/NIREditingPaper.jpg" title="Paper" /></a></td>
		<td><a href="https://yaksoy.github.io/papers/SIGp24-NIREditing.jpg" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/NIREditingPosterSmall.jpg" title="Poster" /></a></td>
		<td><a href="https://youtu.be/tNsUQYI7vw8" target="_blank"><img src="https://yaksoy.github.io/images/research/NIREditingVideo.jpg" title="Video" /></a></td>
	</tr></table>
	</div>

  <input id="SIGp24-item3" name="accordion1" type="checkbox" />
  <label for="SIGp24-item3">BibTeX</label>
  <div class="pub-bibtex">
	@INPROCEEDINGS{NIREditing,<br />
	author={Samuel Antunes Miranda and Shahrzad Mirzaei and Mariam Bebawy and Sebastian Dille and Ya\u{g}{\i}z Aksoy},<br />
	title={Interactive RGB+NIR Photo Editing},<br />
	booktitle={SIGGRAPH Posters},<br />
	year={2024},<br />
	}
  </div>
</div>
</td>
</tr>
</table>

<hr />

<table>
<tr>
<td class="pub-photocol">
<a href="../datamosh" target="_blank"><img src="https://yaksoy.github.io/images/research/datamosh.jpg" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="../datamosh" target="_blank">Datamoshing with Optical Flow <i class="fa fa-external-link"></i></a>
</div>
<div class="pub-authors">
	<b>Chris Careaga</b>, <b>Mahesh Kumar Krishna Reddy</b>, and <b>Yağız Aksoy</b> 
</div>
<div class="pub-venue">
	SIGGRAPH Asia Posters, 2023
</div>

<div class="pub-accordion">
  <input id="SIGa23p-item1" name="accordion1" type="checkbox" />
  <label for="SIGa23p-item1">Abstract</label>
  <div class="pub-abstract">
	We propose a simple method for emulating the effect of data moshing, without relying on the corruption of encoded video, and explore its use in different application scenarios. 
	Like traditional data moshing, we apply motion information to mismatched visual data.
	Our approach uses off-the-shelf optical flow estimation to generate motion vectors for each pixel. 
	Our core algorithm can be implemented in a handful of lines but unlocks multiple video editing effects. 
	The use of accurate optical flow rather than compression data also creates a more natural transition without block artifacts. 
	We hope our method provides artists and content creators with more creative freedom over the process of data moshing.
  </div>

  <input id="SIGa23p-item2" name="accordion1" type="checkbox" />
  <label for="SIGa23p-item2">Manuscript &amp; more</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="https://yaksoy.github.io/papers/SIGa23p-Datamosh.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/datamoshPaper.jpg" title="Paper" /></a></td>
		<td><a href="https://yaksoy.github.io/papers/SIGa23p-Datamosh.jpg" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/datamoshPosterSmall.jpg" title="Poster" /></a></td>
	</tr></table>
	</div>

  <input id="SIGa23p-item3" name="accordion1" type="checkbox" />
  <label for="SIGa23p-item3">BibTeX</label>
  <div class="pub-bibtex">
@INPROCEEDINGS{datamosh,<br />
	author={Chris Careaga and Mahesh Kumar Krishna Reddy and Ya\u{g}{\i}z Aksoy},<br />
	title={Datamoshing with Optical Flow},<br />
	booktitle={SIGGRAPH Asia Posters},<br />
	year={2023},<br />
	}
  </div>
</div>
</td>
</tr>
</table>

<hr />

<table>
<tr>
<td class="pub-photocol">
<a href="../parallaxbg" target="_blank"><img src="https://yaksoy.github.io/images/research/parallax.jpg" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="../parallaxbg" target="_blank">Parallax Background Texture Generation <i class="fa fa-external-link"></i></a>
</div>
<div class="pub-authors">
	Brigham Okano, Shao Yu Shen, <b>Sebastian Dille</b>, and <b>Yağız Aksoy</b> 
</div>
<div class="pub-venue">
	SIGGRAPH Posters, 2022
</div>

<div class="pub-accordion">
  <input id="SIG22c-item1" name="accordion1" type="checkbox" />
  <label for="SIG22c-item1">Abstract</label>
  <div class="pub-abstract">
	Art assets for games can be time intensive to produce. 
	Whether it is a full 3D world, or simpler 2D background, creating good looking assets takes time and skills that are not always readily available. 
	Time can be saved by using repeating assets, but visible repetition hurts immersion. 
	Procedural generation techniques can help make repetition less uniform, but do not remove it entirely. 
	Both approaches leave noticeable levels of repetition in the image, and require significant time and skill investments to produce. 
	Video game developers in hobby, game jam, or early prototyping situations may not have access to the required time and skill. 
	We propose a framework to produce layered 2D backgrounds without the need for significant artist time or skill. 
	In our pipeline, the user provides segmented photographic input, instead of creating traditional art, and receives game-ready assets. 
	By utilizing photographs as input, we can achieve both a high level of realism for the resulting background texture as well as a shift from manual work away towards computational run-time which frees up developers for other work.
  </div>

  <input id="SIG22c-item2" name="accordion1" type="checkbox" />
  <label for="SIG22c-item2">Manuscript &amp; more</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="https://yaksoy.github.io/papers/SIG22c-ParallaxBG.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/parallaxPaper.jpg" title="Paper" /></a></td>
		<td><a href="https://yaksoy.github.io/papers/SIG22c-ParallaxBG.jpg" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/parallaxPosterSmall.jpg" title="Poster" /></a></td>
		<td><a href="https://youtu.be/_KWdFy3YipI" target="_blank"><img src="https://yaksoy.github.io/images/research/parallaxVideo.jpg" title="Video" /></a></td>
	</tr></table>
	</div>

  <input id="SIG22c-item3" name="accordion1" type="checkbox" />
  <label for="SIG22c-item3">BibTeX</label>
  <div class="pub-bibtex">
	@INPROCEEDINGS{parallaxBG,<br />
	author={Brigham Okano and Shao Yu Shen and Sebastian Dille and Ya\u{g}{\i}z Aksoy},<br />
	title={Parallax Background Texture Generation},<br />
	booktitle={SIGGRAPH Posters},<br />
	year={2022},<br />
	}
  </div>
</div>
</td>
</tr>
</table>

<hr />

<table>
<tr>
<td class="pub-photocol">
<a href="../dynapix" target="_blank"><img src="https://yaksoy.github.io/images/research/dynapix.jpg" class="pub-photo" /></a>
</td>
<td>
<div class="pub-title">
	<a href="../dynapix" target="_blank">DynaPix: Normal Map Pixelization for Dynamic Lighting <i class="fa fa-external-link"></i></a>
</div>
<div class="pub-authors">
	Gerardo Gandeaga, Denys Iliash, <b>Chris Careaga</b>, and <b>Yağız Aksoy</b> 
</div>
<div class="pub-venue">
	SIGGRAPH Posters, 2022
</div>

<div class="pub-accordion">
  <input id="SIG22b-item1" name="accordion1" type="checkbox" />
  <label for="SIG22b-item1">Abstract</label>
  <div class="pub-abstract">
	This work introduces DynaPix, a Krita extension that automatically generates pixelated images and surface normals from an input image. 
	DynaPix is a tool that aids pixel artists and game developers more efficiently develop 8-bit style games and bring them to life with dynamic lighting through normal maps that can be used in modern game engines such as Unity. 
	The extension offers artists a degree of flexibility as well as allows for further refinements to generated artwork. 
	Powered by out of the box solutions, DynaPix is a tool that seamlessly integrates in the artistic workflow.
  </div>

  <input id="SIG22b-item2" name="accordion1" type="checkbox" />
  <label for="SIG22b-item2">Manuscript &amp; more</label>
  <div class="pub-photolink">
  
	<table><tr>
		<td><a href="https://yaksoy.github.io/papers/SIG22b-DynaPix.pdf" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/dynapixPaper.jpg" title="Paper" /></a></td>
		<td><a href="https://yaksoy.github.io/papers/SIG22b-DynaPix.jpg" target="_blank"><img width="200" src="https://yaksoy.github.io/images/research/dynapixPosterSmall.jpg" title="Poster" /></a></td>
		<td><a href="https://youtu.be/1mylyzw6i_U" target="_blank"><img src="https://yaksoy.github.io/images/research/dynapixVideo.jpg" title="Video" /></a></td>
	</tr></table>
	</div>

  <input id="SIG22b-item3" name="accordion1" type="checkbox" />
  <label for="SIG22b-item3">BibTeX</label>
  <div class="pub-bibtex">
	@INPROCEEDINGS{dynapix,<br />
	author={Gerardo Gandeaga and Denys Iliash and Chris Careaga and Ya\u{g}{\i}z Aksoy},<br />
	title={Dyna{P}ix: Normal Map Pixelization for Dynamic Lighting},<br />
	booktitle={SIGGRAPH Posters},<br />
	year={2022},<br />
	}
  </div>
</div>
</td>
</tr>
</table>

<hr />

      </div><!-- /.entry-contentProject -->
    </div><!-- /.entry-wrapper -->
  </article>
</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo" class="entry-wrapper">
    

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
<span>&copy; 2025 Yagiz Aksoy</span>
  </footer>
</div><!-- /.footer-wrapper -->

<script type="text/javascript">
  var BASE_URL = 'https://yaksoy.github.io';
</script>

<!-- Include Latex style math -->
<!-- https://stackoverflow.com/questions/10987992/using-mathjax-with-jekyll -->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="https://yaksoy.github.io/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="https://yaksoy.github.io/assets/js/scripts.min.js"></script>



<!-- Start of StatCounter Code for Default Guide -->
<script type="text/javascript">
var sc_project=11195487; 
var sc_invisible=1; 
var sc_security="112d9b46"; 
var scJsHost = (("https:" == document.location.protocol) ?
"https://secure." : "http://www.");
document.write("<sc"+"ript type='text/javascript' src='" +
scJsHost+
"statcounter.com/counter/counter.js'></"+"script>");
</script>
<noscript><div class="statcounter"><a title="shopify
analytics" href="http://statcounter.com/shopify/"
target="_blank"><img class="statcounter"
src="//c.statcounter.com/11195487/0/112d9b46/1/"
alt="shopify analytics"></a></div></noscript>
<!-- End of StatCounter Code for Default Guide -->

</body>
</html>
